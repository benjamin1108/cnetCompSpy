#!/usr/bin/env python3
"""
äº§å“åŠ¨æ€é¡µé¢çˆ¬è™«
ä¸“é—¨çˆ¬å–å„äº‘å‚å•†çš„äº§å“åŠ¨æ€/æ–°åŠŸèƒ½å‘å¸ƒè®°å½•é¡µé¢
"""

import asyncio
import sys
from pathlib import Path
from datetime import datetime
from playwright.async_api import async_playwright
from rich.console import Console
from rich.progress import Progress

# æ·»åŠ  src ç›®å½•åˆ° Python è·¯å¾„
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

from config_loader import config_loader
from src.help_crawler.content_extractor import (
    crawl_and_extract,
    save_content,
)

CONSOLE = Console()
OUTPUT_FORMATS = ['md']

# ç«å±±äº‘äº§å“åŠ¨æ€URLåˆ—è¡¨
VOLCENGINE_WHATSNEW = [
    {"product": "vpc", "name": "VPCç§æœ‰ç½‘ç»œ - äº§å“åŠ¨æ€", "url": "https://www.volcengine.com/docs/6401/67803"},
    {"product": "eip", "name": "å…¬ç½‘IP(EIP) - æ–°åŠŸèƒ½å‘å¸ƒè®°å½•", "url": "https://www.volcengine.com/docs/6402/67934"},
    {"product": "shared_bandwidth_package", "name": "å…±äº«å¸¦å®½åŒ… - æ–°åŠŸèƒ½å‘å¸ƒè®°å½•", "url": "https://www.volcengine.com/docs/6623/93507"},
    {"product": "nat_gateway", "name": "NATç½‘å…³ - æ–°åŠŸèƒ½å‘å¸ƒè®°å½•", "url": "https://www.volcengine.com/docs/6404/67976"},
    {"product": "ipv6_gateway", "name": "IPv6ç½‘å…³ - æ–°åŠŸèƒ½å‘å¸ƒè®°å½•", "url": "https://www.volcengine.com/docs/6896/133698"},
    {"product": "cen", "name": "äº‘ä¼ä¸šç½‘(CEN) - äº§å“åŠ¨æ€", "url": "https://www.volcengine.com/docs/6405/68180"},
    {"product": "transit_router", "name": "ä¸­è½¬è·¯ç”±å™¨(TR) - äº§å“åŠ¨æ€", "url": "https://www.volcengine.com/docs/6979/155961"},
    {"product": "clb", "name": "è´Ÿè½½å‡è¡¡(CLB) - äº§å“åŠ¨æ€", "url": "https://www.volcengine.com/docs/6406/1264527"},
    {"product": "clb_release", "name": "è´Ÿè½½å‡è¡¡(CLB) - æ–°åŠŸèƒ½å‘å¸ƒè®°å½•", "url": "https://www.volcengine.com/docs/6406/113563"},
    {"product": "alb", "name": "åº”ç”¨å‹è´Ÿè½½å‡è¡¡(ALB) - äº§å“åŠ¨æ€", "url": "https://www.volcengine.com/docs/6406/67894"},
    {"product": "direct_connect", "name": "ä¸“çº¿è¿æ¥ - æ–°åŠŸèƒ½å‘å¸ƒè®°å½•", "url": "https://www.volcengine.com/docs/6407/68174"},
    {"product": "vpn_connection", "name": "VPNè¿æ¥ - æ–°åŠŸèƒ½å‘å¸ƒè®°å½•", "url": "https://www.volcengine.com/docs/6455/70535"},
    {"product": "private_link", "name": "ç§ç½‘è¿æ¥(PrivateLink) - æ–°åŠŸèƒ½å‘å¸ƒè®°å½•", "url": "https://www.volcengine.com/docs/6980/155768"},
    {"product": "nic", "name": "ç½‘ç»œæ™ºèƒ½ä¸­å¿ƒ(NIC) - äº§å“åŠ¨æ€", "url": "https://www.volcengine.com/docs/83782/1155035"},
    {"product": "cloud_connector", "name": "äº‘è¿æ¥å™¨ - äº§å“åŠ¨æ€", "url": "https://www.volcengine.com/docs/83812/1167747"},
    {"product": "shared_traffic_package", "name": "å…±äº«æµé‡åŒ… - äº§å“åŠ¨æ€", "url": "https://www.volcengine.com/docs/84435/1282844"},
]

# è…¾è®¯äº‘äº§å“åŠ¨æ€URLåˆ—è¡¨
TENCENTCLOUD_WHATSNEW = [
    {"product": "clb", "name": "è´Ÿè½½å‡è¡¡(CLB) - äº§å“åŠ¨æ€", "url": "https://cloud.tencent.com/document/product/214/42841"},
    {"product": "gwlb", "name": "ç½‘å…³è´Ÿè½½å‡è¡¡(GWLB) - äº§å“åŠ¨æ€", "url": "https://cloud.tencent.com/document/product/1782/111599"},
    {"product": "vpc", "name": "ç§æœ‰ç½‘ç»œ(VPC) - äº§å“åŠ¨æ€", "url": "https://cloud.tencent.com/document/product/215/37951"},
    {"product": "nat", "name": "NATç½‘å…³ - äº§å“åŠ¨æ€", "url": "https://cloud.tencent.com/document/product/552/71560"},
    {"product": "bwp", "name": "å…±äº«å¸¦å®½åŒ…(BWP) - äº§å“åŠ¨æ€", "url": "https://cloud.tencent.com/document/product/684/74390"},
    {"product": "stp", "name": "å…±äº«æµé‡åŒ… - äº§å“åŠ¨æ€", "url": "https://cloud.tencent.com/document/product/1171/61165"},
    {"product": "ipv6", "name": "å¼¹æ€§å…¬ç½‘IPv6 - äº§å“åŠ¨æ€", "url": "https://cloud.tencent.com/document/product/1142/112767"},
    {"product": "eip", "name": "å¼¹æ€§å…¬ç½‘IP(EIP) - äº§å“åŠ¨æ€", "url": "https://cloud.tencent.com/document/product/1199/44150"},
    {"product": "ihpn", "name": "æ™ºèƒ½é«˜æ€§èƒ½ç½‘ç»œ(IHPN) - äº§å“åŠ¨æ€", "url": "https://cloud.tencent.com/document/product/1779/110928"},
    {"product": "privatelink", "name": "ç§æœ‰è¿æ¥(PrivateLink) - äº§å“åŠ¨æ€", "url": "https://cloud.tencent.com/document/product/1451/57261"},
    {"product": "dc", "name": "ä¸“çº¿æ¥å…¥(DC) - äº§å“åŠ¨æ€", "url": "https://cloud.tencent.com/document/product/216/52940"},
    {"product": "ccn", "name": "äº‘è”ç½‘(CCN) - äº§å“åŠ¨æ€", "url": "https://cloud.tencent.com/document/product/877/58511"},
    {"product": "vpn", "name": "VPNè¿æ¥ - äº§å“åŠ¨æ€", "url": "https://cloud.tencent.com/document/product/554/53881"},
    {"product": "gaap", "name": "å…¨çƒåº”ç”¨åŠ é€Ÿ(GAAP) - äº§å“åŠ¨æ€", "url": "https://cloud.tencent.com/document/product/608/64952"},
]


async def crawl_whatsnew(vendor: str, whatsnew_list: list):
    """çˆ¬å–æŒ‡å®šå‚å•†çš„äº§å“åŠ¨æ€é¡µé¢"""
    content_base_dir = Path("out/content")
    output_dir = content_base_dir / vendor / "whatsnew"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è·å–å‚å•†é…ç½®
    vendor_config = config_loader.get_vendor_config(vendor)
    crawler_settings = vendor_config.get('crawler_settings', {}) if vendor_config else {}
    save_raw_html = crawler_settings.get('save_raw_html', False)
    
    CONSOLE.print(f"\n[bold cyan]å¼€å§‹çˆ¬å– {vendor} äº§å“åŠ¨æ€é¡µé¢...[/bold cyan]")
    CONSOLE.print(f"[dim]å…± {len(whatsnew_list)} ä¸ªé¡µé¢å¾…çˆ¬å–[/dim]")
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        with Progress(*Progress.get_default_columns(), console=CONSOLE) as progress:
            task = progress.add_task(f"[green]çˆ¬å– {vendor} äº§å“åŠ¨æ€", total=len(whatsnew_list))
            
            for item in whatsnew_list:
                try:
                    extracted_data = await crawl_and_extract(page, item['url'], vendor, save_raw_html)
                    if extracted_data:
                        # ä½¿ç”¨äº§å“åç§°ä½œä¸ºæ ‡é¢˜ï¼Œç¡®ä¿æ¯ä¸ªäº§å“ä¿å­˜ä¸ºç‹¬ç«‹æ–‡ä»¶
                        full_metadata = {
                            "url": item['url'],
                            "vendor": vendor,
                            "product": "whatsnew",
                            "product_key": item['product'],
                            "crawl_time": datetime.now().isoformat(),
                            **extracted_data,  # å…ˆå±•å¼€extracted_data
                            "title": f"{item['product']}_{item['name']}",  # å†è®¾ç½®æ ‡é¢˜ï¼Œè¦†ç›–extracted_dataä¸­çš„title
                        }
                        save_content(content_base_dir, full_metadata, OUTPUT_FORMATS, save_raw_html)
                        CONSOLE.log(f"[green]âœ”[/green] {item['name']}")
                    else:
                        CONSOLE.log(f"[yellow]âš [/yellow] {item['name']} - æ— æ³•æå–å†…å®¹")
                except Exception as e:
                    CONSOLE.log(f"[red]âœ˜[/red] {item['name']} - é”™è¯¯: {e}")
                
                progress.update(task, advance=1)
        
        await browser.close()
    
    CONSOLE.print(f"[bold green]âœ” å®Œæˆ {vendor} äº§å“åŠ¨æ€çˆ¬å–[/bold green]")


async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="äº§å“åŠ¨æ€é¡µé¢çˆ¬è™«")
    parser.add_argument('--vendor', choices=['volcengine', 'tencentcloud', 'all'], 
                       default='all', help='æŒ‡å®šè¦çˆ¬å–çš„å‚å•†')
    args = parser.parse_args()
    
    CONSOLE.print("[bold yellow]ğŸš€ äº§å“åŠ¨æ€é¡µé¢çˆ¬è™«[/bold yellow]")
    CONSOLE.print(f"[dim]å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}[/dim]\n")
    
    if args.vendor in ['volcengine', 'all']:
        await crawl_whatsnew('volcengine', VOLCENGINE_WHATSNEW)
    
    if args.vendor in ['tencentcloud', 'all']:
        await crawl_whatsnew('tencentcloud', TENCENTCLOUD_WHATSNEW)
    
    CONSOLE.print(f"\n[bold green]ğŸ‰ çˆ¬å–å®Œæˆï¼[/bold green]")
    CONSOLE.print(f"[dim]ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}[/dim]")
    CONSOLE.print(f"[dim]è¾“å‡ºç›®å½•: out/content/{{vendor}}/whatsnew/[/dim]")


if __name__ == "__main__":
    asyncio.run(main())





import asyncio
import os
import re
import yaml
import pandas as pd
from pathlib import Path
from datetime import datetime
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from markdownify import markdownify as md
from urllib.parse import urljoin
from io import StringIO
from rich.console import Console

CONSOLE = Console()

class BaseExtractor:
    """
    æå–å™¨åŸºç±»ï¼Œå®šä¹‰äº†æ‰€æœ‰æå–å™¨åº”éµå¾ªçš„æ¥å£å’Œé»˜è®¤å®ç°ã€‚
    """
    def __init__(self, soup: BeautifulSoup, url: str):
        self.soup = soup
        self.url = url

    def extract(self) -> dict:
        """
        æ‰§è¡Œæå–è¿‡ç¨‹å¹¶è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ•°æ®çš„ç»“æ„åŒ–å­—å…¸ã€‚
        """
        title = self._extract_title()
        content_html = self._extract_content_html()
        
        if content_html:
            self._fix_relative_urls(content_html)

        return {
            "title": title,
            "content_html": str(content_html) if content_html else "",
        }

    def _extract_title(self) -> str:
        """æå–é¡µé¢ä¸»æ ‡é¢˜ã€‚é»˜è®¤å®ç°æ˜¯æŸ¥æ‰¾ç¬¬ä¸€ä¸ªh1æ ‡ç­¾ã€‚"""
        title_tag = self.soup.find('h1')
        return title_tag.get_text(strip=True) if title_tag else "Untitled"

    def _extract_content_html(self) -> BeautifulSoup:
        """
        é»˜è®¤çš„å†…å®¹æå–é€»è¾‘ã€‚
        å®ƒä¼šå…ˆç§»é™¤é€šç”¨å¹²æ‰°æ ‡ç­¾ï¼Œç„¶åè¿”å›æ¸…ç†åçš„ <body>ã€‚
        """
        body = self.soup.find('body')
        if not body:
            return self.soup # å¦‚æœæ²¡æœ‰bodyï¼Œè¿”å›æ•´ä¸ªsoup

        # å¯¹bodyè¿›è¡Œé€šç”¨æ¸…ç†
        for tag in body.find_all(['nav', 'footer', 'header', 'script', 'style', 'aside', 'form']):
            tag.decompose()
        return body

    def _fix_relative_urls(self, content_soup: BeautifulSoup):
        """å°†å†…å®¹ä¸­çš„ç›¸å¯¹URLè½¬æ¢ä¸ºç»å¯¹URLã€‚"""
        for tag in content_soup.find_all(['a'], href=True):
            if tag['href'].startswith('/'):
                tag['href'] = urljoin(self.url, tag['href'])
        for tag in content_soup.find_all(['img'], src=True):
            if tag['src'].startswith('/'):
                tag['src'] = urljoin(self.url, tag['src'])


class TencentCloudExtractor(BaseExtractor):
    """è…¾è®¯äº‘ä¸“å±æå–å™¨ã€‚"""
    def _extract_content_html(self) -> BeautifulSoup:
        return self.soup.select_one('#docArticleContent') or super()._extract_content_html()

class AliyunExtractor(BaseExtractor):
    """é˜¿é‡Œäº‘ä¸“å±æå–å™¨ã€‚"""
    def _extract_content_html(self) -> BeautifulSoup:
        return self.soup.select_one('.content-body') or super()._extract_content_html()

class HuaweiCloudExtractor(BaseExtractor):
    """åä¸ºäº‘ä¸“å±æå–å™¨ã€‚"""
    def _extract_content_html(self) -> BeautifulSoup:
        return self.soup.select_one('.content-body') or super()._extract_content_html()

class VolcengineExtractor(BaseExtractor):
    """ç«å±±å¼•æ“ä¸“å±æå–å™¨ã€‚"""
    
    def _convert_volcengine_links(self, content: BeautifulSoup) -> None:
        """
        å°†ç«å±±äº‘ç‰¹æ®Šçš„é“¾æ¥æ ¼å¼è½¬æ¢ä¸ºæ ‡å‡†çš„<a href>æ ‡ç­¾ã€‚
        ç«å±±äº‘ä½¿ç”¨ class="url hyperlink-href:https://..." æ ¼å¼å­˜å‚¨é“¾æ¥ã€‚
        """
        import re
        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å« hyperlink-href çš„å…ƒç´ 
        for elem in content.find_all(class_=re.compile(r'hyperlink-href:')):
            class_str = ' '.join(elem.get('class', []))
            # æå–URL
            match = re.search(r'hyperlink-href:(\S+)', class_str)
            if match:
                href = match.group(1)
                # è·å–æ–‡æœ¬å†…å®¹
                text = elem.get_text(strip=True)
                # åˆ›å»ºæ–°çš„<a>æ ‡ç­¾ï¼Œä½¿ç”¨self.soupè€Œä¸æ˜¯content
                new_tag = self.soup.new_tag('a', href=href)
                new_tag.string = text
                elem.replace_with(new_tag)
    
    def _extract_content_html(self) -> BeautifulSoup:
        # ç«å±±äº‘æ–‡æ¡£çš„ä¸»å†…å®¹åŒºåŸŸé€‰æ‹©å™¨
        # ç«å±±äº‘ä½¿ç”¨åŠ¨æ€ç”Ÿæˆçš„classåï¼Œä½†æœ‰å›ºå®šçš„volc-doceditorç±»class
        selectors = [
            '.volc-doceditor-container',  # æ–‡æ¡£ç¼–è¾‘å™¨å®¹å™¨
            '.editor-kit-container.volc-doceditor',  # ç¼–è¾‘å™¨å®¹å™¨
            '.volc-doceditor',  # æ–‡æ¡£ç¼–è¾‘å™¨
            'article',  # æ–‡ç« æ ‡ç­¾
        ]
        
        for selector in selectors:
            content = self.soup.select_one(selector)
            if content:
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                for tag in content.find_all(['script', 'style']):
                    tag.decompose()
                # è½¬æ¢ç«å±±äº‘ç‰¹æ®Šé“¾æ¥æ ¼å¼
                self._convert_volcengine_links(content)
                return content
        
        # å›é€€åˆ°é»˜è®¤å®ç°
        return super()._extract_content_html()

class DefaultExtractor(BaseExtractor):
    """é»˜è®¤æå–å™¨ï¼Œå½“æ²¡æœ‰ç‰¹å®šå‚å•†çš„æå–å™¨æ—¶ä½¿ç”¨ã€‚"""
    pass


def get_extractor(vendor: str, soup: BeautifulSoup, url: str) -> BaseExtractor:
    """
    æå–å™¨å·¥å‚å‡½æ•°ã€‚æ ¹æ®å‚å•†åç§°è¿”å›ç›¸åº”çš„æå–å™¨å®ä¾‹ã€‚
    """
    extractors = {
        'tencentcloud': TencentCloudExtractor,
        'aliyun': AliyunExtractor,
        'huaweicloud': HuaweiCloudExtractor,
        'volcengine': VolcengineExtractor,
    }
    
    extractor_class = extractors.get(vendor.lower(), DefaultExtractor)
    return extractor_class(soup, url)


def advanced_html_to_markdown(html_content: str) -> str:
    """
    ä¸€ä¸ªå¢å¼ºç‰ˆçš„HTMLåˆ°Markdownè½¬æ¢å™¨ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚è¡¨æ ¼ã€‚
    å®ƒä½¿ç”¨pandasæ¥è§£æè¡¨æ ¼ï¼Œä»è€Œæ­£ç¡®å¤„ç†rowspanå’Œcolspanã€‚
    """
    if not html_content:
        return ""

    soup = BeautifulSoup(html_content, 'html.parser')
    
    # å…ˆå°†è¡¨æ ¼ä¸­çš„é“¾æ¥è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œè¿™æ ·pandaså¤„ç†åä¸ä¼šä¸¢å¤±é“¾æ¥
    for table in soup.find_all('table'):
        for link in table.find_all('a', href=True):
            href = link.get('href', '')
            text = link.get_text(strip=True)
            if href and text:
                # å°†<a>æ ‡ç­¾æ›¿æ¢ä¸ºMarkdowné“¾æ¥æ ¼å¼
                md_link = f'[{text}]({href})'
                link.replace_with(md_link)

    for table_idx, table in enumerate(soup.find_all('table')):
        try:
            # æ£€æŸ¥è¡¨æ ¼æ˜¯å¦æœ‰çœŸæ­£çš„è¡¨å¤´ï¼ˆthæ ‡ç­¾ï¼‰
            has_real_header = bool(table.find('th'))
            
            # ä½¿ç”¨pandasè¯»å–HTMLè¡¨æ ¼
            # å¦‚æœæ²¡æœ‰çœŸæ­£çš„è¡¨å¤´ï¼Œå‘Šè¯‰pandasä¸è¦æŠŠç¬¬ä¸€è¡Œå½“ä½œheader
            if has_real_header:
                df_list = pd.read_html(StringIO(str(table)), flavor='bs4')
            else:
                df_list = pd.read_html(StringIO(str(table)), flavor='bs4', header=None)
            
            if not df_list:
                continue
            
            # ä¸€ä¸ª<table>æ ‡ç­¾é€šå¸¸åªåŒ…å«ä¸€ä¸ªè¡¨æ ¼
            df = df_list[0]
            
            # æ•°æ®æ¸…ç†ï¼š
            # 1. åˆ é™¤å…¨ç©ºçš„è¡Œ
            df = df.dropna(how='all')
            
            # 2. åˆ é™¤å…¨ç©ºçš„åˆ—
            df = df.dropna(axis=1, how='all')
            
            # 3. å°†NaNæ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
            df = df.fillna('')
            
            # 4. åˆ é™¤æ‰€æœ‰åˆ—éƒ½æ˜¯ç©ºå­—ç¬¦ä¸²çš„è¡Œ
            df = df[~(df == '').all(axis=1)]
            
            # 5. å¤„ç†colspanäº§ç”Ÿçš„åˆ—é”™ä½é—®é¢˜ï¼šå°†éƒ¨åˆ†ç©ºåˆ—çš„å†…å®¹åˆå¹¶åˆ°å‰ä¸€åˆ—
            cols_to_drop = []
            for col_idx in range(len(df.columns) - 1, 0, -1):  # ä»å³åˆ°å·¦éå†
                col = df.columns[col_idx]
                prev_col = df.columns[col_idx - 1]
                # å¦‚æœå½“å‰åˆ—å¤§éƒ¨åˆ†æ˜¯ç©ºçš„ï¼ˆè¶…è¿‡50%ï¼‰ï¼Œå°†éç©ºå†…å®¹åˆå¹¶åˆ°å‰ä¸€åˆ—
                empty_count = (df[col] == '').sum()
                if empty_count > len(df) * 0.5:  # è¶…è¿‡50%æ˜¯ç©ºçš„
                    for idx in df.index:
                        if df.at[idx, col] != '' and df.at[idx, prev_col] != '':
                            # åˆå¹¶åˆ°å‰ä¸€åˆ—
                            df.at[idx, prev_col] = str(df.at[idx, prev_col]) + str(df.at[idx, col])
                        elif df.at[idx, col] != '':
                            df.at[idx, prev_col] = df.at[idx, col]
                    cols_to_drop.append(col)
            
            # åˆ é™¤å·²å¤„ç†çš„åˆ—
            if cols_to_drop:
                df = df.drop(columns=cols_to_drop)
            
            # 6. åˆ é™¤æ‰€æœ‰å€¼éƒ½æ˜¯ç©ºå­—ç¬¦ä¸²çš„åˆ—
            df = df.loc[:, ~(df == '').all()]
            
            # 7. ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœç¬¬ä¸€è¡Œæ˜¯ç©ºçš„ï¼ˆå¯èƒ½æ˜¯pandasç”Ÿæˆçš„ç©ºè¡¨å¤´ï¼‰ï¼Œåˆ é™¤å®ƒ
            if not df.empty and (df.iloc[0] == '').all():
                df = df.iloc[1:].reset_index(drop=True)
            
            # 8. å¦‚æœDataFrameä¸ºç©ºï¼Œè·³è¿‡è¿™ä¸ªè¡¨æ ¼
            if df.empty:
                continue
            
            # å°†DataFrameè½¬æ¢ä¸ºä¸€ä¸ªç®€å•çš„ã€æ²¡æœ‰åˆå¹¶å•å…ƒæ ¼çš„HTMLè¡¨æ ¼
            # header=False ç¡®ä¿ä¸è¾“å‡ºpandasè‡ªåŠ¨ç”Ÿæˆçš„åˆ—å
            if has_real_header:
                simple_table_html = df.to_html(index=False, border=0, escape=False)
            else:
                simple_table_html = df.to_html(index=False, border=0, header=False, escape=False)
            
            simple_table_soup = BeautifulSoup(simple_table_html, 'html.parser')
            new_table = simple_table_soup.find('table')
            
            # å…³é”®ä¿®å¤ï¼šå°†ç¬¬ä¸€è¡Œçš„tdè½¬æ¢ä¸ºthï¼Œè¿™æ ·markdownifyå°±èƒ½æ­£ç¡®è¯†åˆ«è¡¨å¤´
            first_row = new_table.find('tr')
            if first_row:
                for td in first_row.find_all('td'):
                    # åˆ›å»ºæ–°çš„thæ ‡ç­¾ï¼Œå¤åˆ¶tdçš„å†…å®¹å’Œå±æ€§
                    th = simple_table_soup.new_tag('th')
                    th.string = td.get_text()
                    # å¤åˆ¶æ‰€æœ‰å±æ€§
                    for attr, value in td.attrs.items():
                        th[attr] = value
                    # æ›¿æ¢tdä¸ºth
                    td.replace_with(th)
            
            # æ›¿æ¢æ—§çš„å¤æ‚è¡¨æ ¼
            table.replace_with(new_table)

        except Exception as e:
            # å¦‚æœpandaså¤„ç†å¤±è´¥ï¼ˆä¾‹å¦‚ï¼Œè¡¨æ ¼æ ¼å¼éå¸¸ä¸è§„èŒƒï¼‰ï¼Œ
            # æ‰“å°ä¸€ä¸ªè­¦å‘Šå¹¶ä¿æŒåŸæ ·ï¼Œè®©markdownifyå°è¯•å¤„ç†
            CONSOLE.log(f"[yellow]è­¦å‘Š: å¤„ç†è¡¨æ ¼æ—¶å‡ºé”™: {e}ã€‚å°†å›é€€åˆ°é»˜è®¤è½¬æ¢ã€‚[/yellow]")
            continue

    # å°†æ•´ä¸ªHTMLï¼ˆç°åœ¨åªåŒ…å«ç®€å•è¡¨æ ¼ï¼‰è½¬æ¢ä¸ºMarkdown
    md_content = md(str(soup), heading_style="ATX", bullets='-')

    return md_content


def parse_link_file(file_path: Path):
    """ä»é“¾æ¥æ–‡ä»¶ä¸­è§£æURLå’Œæ ‡é¢˜ã€‚"""
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    docs = []
    current_title = None
    item_pattern = re.compile(r"^\s*\d+\.\s*(.*)")

    try:
        start_index = lines.index("=" * 50 + "\n") + 2
    except ValueError:
        start_index = 0

    for i in range(start_index, len(lines)):
        line = lines[i].strip()
        if not line:
            continue
        
        match = item_pattern.match(line)
        if match:
            current_title = match.group(1).strip()
        elif (line.startswith("https://") or line.startswith("http://")) and current_title:
            docs.append({"title": current_title, "url": line})
            current_title = None
        elif current_title: # å¤„ç†å¤šè¡Œæ ‡é¢˜
             current_title += " " + line
                
    return docs


def create_metadata_header(metadata: dict) -> str:
    """åˆ›å»ºYAML Front Matteræ ¼å¼çš„å…ƒæ•°æ®å—ã€‚"""
    # è¿‡æ»¤æ‰å†…å®¹å­—æ®µï¼Œåªä¿ç•™å…ƒæ•°æ®
    excluded_fields = ['md_content', 'txt_content', 'raw_html']
    header_data = {k: v for k, v in metadata.items() if k not in excluded_fields}
    return f"---\n{yaml.dump(header_data, allow_unicode=True)}---\n\n"


async def crawl_and_extract(page, url: str, vendor: str, save_raw_html: bool = False):
    """
    è·å–é¡µé¢HTMLï¼Œå¹¶ä½¿ç”¨é€‚åˆè¯¥å‚å•†çš„æå–å™¨æ¥å¤„ç†å®ƒã€‚
    """
    try:
        response = await page.goto(url, timeout=60000, wait_until='domcontentloaded')
        
        # ç«å±±äº‘æ˜¯SPAé¡µé¢ï¼Œéœ€è¦ç­‰å¾…JavaScriptæ¸²æŸ“å®Œæˆ
        if vendor.lower() == 'volcengine':
            # ç­‰å¾…å†…å®¹å®¹å™¨å‡ºç°ï¼Œè¿™æ¯”networkidleæ›´ç²¾ç¡®
            try:
                await page.wait_for_selector('.volc-doceditor-container', timeout=15000)
            except:
                # å¦‚æœå®¹å™¨æ²¡å‡ºç°ï¼Œå›é€€åˆ°çŸ­æš‚çš„networkidleç­‰å¾…
                try:
                    await page.wait_for_load_state('networkidle', timeout=10000)
                except:
                    pass
            # çŸ­æš‚ç­‰å¾…ç¡®ä¿å†…å®¹æ¸²æŸ“
            await page.wait_for_timeout(500)
        
        # è·å–æ¸²æŸ“åçš„HTMLå†…å®¹
        html_content = await page.content()
        html_bytes = html_content.encode('utf-8')
        soup = BeautifulSoup(html_bytes, 'lxml')

        # å¦‚æœå¯ç”¨äº†è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜åŸå§‹HTML
        raw_html_content = None
        if save_raw_html:
            raw_html_content = html_content

        # ä½¿ç”¨å·¥å‚å‡½æ•°è·å–åˆé€‚çš„æå–å™¨
        extractor = get_extractor(vendor, soup, url)
        extracted_data = extractor.extract()

        # å°†HTMLå†…å®¹è½¬æ¢ä¸ºMarkdownå’ŒTXT
        content_html = extracted_data.get('content_html', '')

        # ä½¿ç”¨æˆ‘ä»¬æ–°çš„ã€æ›´å¼ºå¤§çš„HTMLåˆ°Markdownè½¬æ¢å‡½æ•°
        md_content = advanced_html_to_markdown(content_html)
        
        txt_content = BeautifulSoup(content_html, 'html.parser').get_text(separator='\\n', strip=True)
        
        # æ¸…ç†ä¸éœ€è¦çš„Unicodeå­—ç¬¦ï¼ˆä¾‹å¦‚ï¼šé›¶å®½éä¸­æ–­ç©ºæ ¼ U+FEFFï¼‰
        if md_content:
            md_content = md_content.replace('\ufeff', '')
        if txt_content:
            txt_content = txt_content.replace('\ufeff', '')
        
        result = {
            "title": extracted_data.get('title'),
            "md_content": md_content,
            "txt_content": txt_content,
        }
        
        # å¦‚æœå¯ç”¨äº†è°ƒè¯•æ¨¡å¼ï¼Œå°†åŸå§‹HTMLæ·»åŠ åˆ°ç»“æœä¸­
        if save_raw_html and raw_html_content:
            result["raw_html"] = raw_html_content
        
        return result
    except Exception as e:
        CONSOLE.log(f"[red]âŒ çˆ¬å– {url} æ—¶å‡ºé”™: {e}[/red]")
        return None


def save_content(output_dir: Path, metadata: dict, output_formats: list = ['md'], save_raw_html: bool = False):
    """å°†æå–çš„å†…å®¹å’Œå…ƒæ•°æ®ä¿å­˜ä¸ºæ–‡ä»¶ã€‚"""
    vendor = metadata.get('vendor', 'unknown')
    product = metadata.get('product', 'unknown')
    
    safe_title = re.sub(r'[\\/*?:"<>|]', "", metadata['title'])
    safe_filename = safe_title.replace(" ", "_")[:100]

    metadata_header = create_metadata_header(metadata)
    
    content_map = {
        'md': metadata.get('md_content', ''),
        'txt': metadata.get('txt_content', '')
    }

    target_dir = output_dir / vendor / product
    target_dir.mkdir(parents=True, exist_ok=True)
        
    for format_type in output_formats:
        content_to_save = content_map.get(format_type)
        if content_to_save:
            file_path = target_dir / f"{safe_filename}.{format_type}"
            try:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(metadata_header + content_to_save)
            except Exception as e:
                CONSOLE.log(f"[red]âŒ ä¿å­˜æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}[/red]")
    
    # å¦‚æœå¯ç”¨äº†è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜åŸå§‹HTML
    if save_raw_html and metadata.get('raw_html'):
        debug_dir = output_dir / 'debug' / vendor / product
        debug_dir.mkdir(parents=True, exist_ok=True)
        
        html_file_path = debug_dir / f"{safe_filename}.html"
        try:
            with open(html_file_path, 'w', encoding='utf-8') as f:
                f.write(metadata['raw_html'])
            CONSOLE.log(f"[green]âœ… å·²ä¿å­˜åŸå§‹HTML: {html_file_path}[/green]")
        except Exception as e:
            CONSOLE.log(f"[red]âŒ ä¿å­˜åŸå§‹HTML {html_file_path} æ—¶å‡ºé”™: {e}[/red]") 