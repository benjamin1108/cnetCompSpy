import datetime
import logging
import os
import sys
import argparse # å¯¼å…¥argparseç”¨äºå¤„ç†å‘½ä»¤è¡Œå‚æ•°
import re # <--- Added import re
from typing import Dict, Optional, Any, List, Tuple # Added List and Tuple

# --- åŠ¨æ€å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°sys.path ---
# ç¡®ä¿å½“è„šæœ¬ä»'scripts'ç›®å½•æˆ–å…¶ä»–ä½ç½®è¿è¡Œæ—¶ï¼Œèƒ½å¤Ÿæ‰¾åˆ°'src'æ¨¡å—ã€‚
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„ã€‚
_SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# å‡è®¾é¡¹ç›®æ ¹ç›®å½•æ˜¯'scripts'ç›®å½•çš„çˆ¶ç›®å½•ã€‚
_PROJECT_ROOT = os.path.dirname(_SCRIPT_DIR)
# å¦‚æœé¡¹ç›®æ ¹ç›®å½•ä¸åœ¨Pythonè·¯å¾„ä¸­ï¼Œåˆ™æ·»åŠ åˆ°è·¯å¾„åˆ—è¡¨çš„å¼€å¤´ã€‚
if _PROJECT_ROOT not in sys.path:
    sys.path.insert(0, _PROJECT_ROOT) # æ’å…¥åˆ°å¼€å¤´ä»¥ä¼˜å…ˆåŠ è½½é¡¹ç›®æ¨¡å—
# --- sys.path ä¿®æ”¹ç»“æŸ ---

from src.utils.config_loader import get_config 
from src.utils.colored_logger import setup_colored_logging

from src.ai_analyzer.model_manager import ModelManager
from src.ai_analyzer.exceptions import AIAnalyzerError, APIError 

def load_prompt_template(prompt_file_path: str) -> str:
    """ä»ç»™å®šæ–‡ä»¶åŠ è½½æç¤ºæ¨¡æ¿ã€‚"""
    logger = logging.getLogger(__name__)
    try:
        with open(prompt_file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        logger.error(f"æç¤ºæ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {prompt_file_path}")
        raise
    except Exception as e:
        logger.error(f"è¯»å–æç¤ºæ¨¡æ¿æ–‡ä»¶ {prompt_file_path} æ—¶å‡ºé”™: {e}")
        raise

def get_all_raw_content_for_week(config: dict) -> List[Dict[str, str]]:
    """
    è·å–å½“å‰å‘¨ï¼ˆå‘¨ä¸€è‡³å‘¨æ—¥ï¼‰æ‰€æœ‰åŸå§‹æ–‡ç«  (.md) çš„å†…å®¹åŠå…ƒæ•°æ®ã€‚
    æ—¥æœŸä»æ–‡ä»¶åä¸­è§£æ (ä¾‹å¦‚ YYYY_MM_DD_*.md)ã€‚
    æ¯ç¯‡æ–‡ç« ä¿¡æ¯ä»¥å­—å…¸å½¢å¼è¿”å›ï¼ŒåŒ…å«åŸå§‹å†…å®¹ã€å‚å•†ã€å­ç±»åˆ«ã€åŸå§‹æ–‡ä»¶åç­‰ã€‚
    å‡è®¾åŸæ–‡URLåœ¨.mdæ–‡ä»¶çš„ç¬¬ä¸€è¡Œã€‚
    """
    logger = logging.getLogger(__name__)
    raw_data_config = config.get("data_paths", {})
    base_path = raw_data_config.get("raw_articles_base", "data/raw")
    vendors = config.get("vendors_to_scan", ["aws", "azure", "gcp"])

    if not os.path.isdir(base_path):
        logger.error(f"åŸå§‹æ•°æ®åŸºç¡€è·¯å¾„æœªæ‰¾åˆ°æˆ–ä¸æ˜¯ä¸€ä¸ªç›®å½•: {base_path}")
        return []

    today = datetime.date.today()
    start_of_week = today - datetime.timedelta(days=today.weekday())
    
    all_articles_data = [] # Changed variable name
    logger.info(f"æ­£åœ¨æ‰«æä» {start_of_week.isoformat()} åˆ° {today.isoformat()} çš„åŸå§‹æ–‡ç« ï¼Œä¾›åº”å•†: {vendors}")

    for vendor in vendors:
        vendor_base_path = os.path.join(base_path, vendor)
        if not os.path.isdir(vendor_base_path):
            logger.debug(f"ä¾›åº”å•†åŸºç¡€è·¯å¾„æœªæ‰¾åˆ°æˆ–ä¸æ˜¯ç›®å½•ï¼Œè·³è¿‡: {vendor_base_path}")
            continue

        try:
            subcategories = [d for d in os.listdir(vendor_base_path) if os.path.isdir(os.path.join(vendor_base_path, d))]
            if not subcategories:
                logger.debug(f"åœ¨ {vendor_base_path} ä¸­æœªæ‰¾åˆ°å­ç±»åˆ«ï¼Œè·³è¿‡æ­¤ä¾›åº”å•†ã€‚")
                continue
        except OSError as e_list_sub:
            logger.warning(f"æ— æ³•åˆ—å‡º {vendor_base_path} ä¸­çš„å­ç±»åˆ«: {e_list_sub}")
            continue
        
        for subcategory in subcategories:
            subcategory_path = os.path.join(vendor_base_path, subcategory)
            logger.info(f"æ­£åœ¨æ‰«æç›®å½•: {subcategory_path}") 

            try:
                for filename in os.listdir(subcategory_path):
                    if filename.endswith(".md"):
                        try:
                            date_part_str = filename.split('_', 3)
                            if len(date_part_str) >= 3:
                                file_date_str = f"{date_part_str[0]}-{date_part_str[1]}-{date_part_str[2]}"
                                file_date_obj = datetime.datetime.strptime(file_date_str, "%Y-%m-%d").date()
                                
                                if start_of_week <= file_date_obj <= today:
                                    is_target_day_for_week = False
                                    for i in range(today.weekday() + 1):
                                        current_iter_day = start_of_week + datetime.timedelta(days=i)
                                        if file_date_obj == current_iter_day:
                                            is_target_day_for_week = True
                                            break
                                    
                                    if is_target_day_for_week:
                                        file_path = os.path.join(subcategory_path, filename)
                                        try:
                                            with open(file_path, 'r', encoding='utf-8') as f_article:
                                                lines = f_article.readlines()
                                            
                                            original_url = ""
                                            raw_content_for_llm_lines = []
                                            # metadata_header_ended = False # Not strictly needed with new logic

                                            in_metadata_section = True 
                                            
                                            for line_idx, line_content in enumerate(lines):
                                                stripped_line = line_content.strip()
                                                
                                                if original_url == "" and stripped_line.startswith("**åŸå§‹é“¾æ¥:**"):
                                                    match = re.search(r'\((https?://[^\)]+)\)', stripped_line) # Corrected regex escaping for ( and )
                                                    if match:
                                                        original_url = match.group(1)
                                                        logger.debug(f"æ–‡ä»¶ {filename}: ä»'**åŸå§‹é“¾æ¥:**'è¡Œé€šè¿‡æ­£åˆ™æ‰¾åˆ°URL: {original_url}")
                                                    else:
                                                        potential_url = stripped_line.replace("**åŸå§‹é“¾æ¥:**", "").strip()
                                                        if potential_url.startswith("http://") or potential_url.startswith("https://"):
                                                            original_url = potential_url # This case is less likely if markdown link is used
                                                            logger.debug(f"æ–‡ä»¶ {filename}: ä»'**åŸå§‹é“¾æ¥:**'è¡Œç›´æ¥æå–URL (éæ­£åˆ™åŒ¹é…): {original_url}")
                                                
                                                if stripped_line == "---" and in_metadata_section:
                                                    in_metadata_section = False
                                                    continue 
                                                
                                                if not in_metadata_section:
                                                    raw_content_for_llm_lines.append(line_content)

                                            raw_content_for_llm = "".join(raw_content_for_llm_lines)

                                            # Fallback logic if new parsing fails or "---" delimiter is missing
                                            if not original_url and lines: 
                                                first_line_stripped = lines[0].strip()
                                                if first_line_stripped.startswith("http://") or first_line_stripped.startswith("https://"):
                                                    original_url = first_line_stripped
                                                    logger.debug(f"æ–‡ä»¶ {filename}: åå¤‡é€»è¾‘ - ä»ç¬¬ä¸€è¡Œæå–åˆ°URL: {original_url}")
                                                    if not raw_content_for_llm.strip(): 
                                                       raw_content_for_llm = "".join(lines[1:])
                                                # If raw_content_for_llm is still empty (meaning "---" was not found and first line wasn't URL)
                                                # then the whole file is content, and URL remains empty (or as found by first line check)
                                                elif not raw_content_for_llm.strip(): 
                                                    logger.debug(f"æ–‡ä»¶ {filename}: åå¤‡é€»è¾‘ - ç¬¬ä¸€è¡Œä¸æ˜¯URLï¼Œä¸”æœªæ‰¾åˆ°'---'åˆ†éš”ç¬¦ã€‚å°†æ•´ä¸ªæ–‡ä»¶è§†ä¸ºåŸå§‹å†…å®¹ã€‚URLä¿æŒä¸º: '{original_url if original_url else 'æœªæ‰¾åˆ°'}'")
                                                    raw_content_for_llm = "".join(lines)


                                            if not raw_content_for_llm.strip():
                                                logger.warning(f"æ–‡ä»¶ {filename} çš„æœ‰æ•ˆåŸå§‹å†…å®¹ä¸ºç©ºï¼ˆå¯èƒ½åœ¨å…ƒæ•°æ®æå–åï¼‰ï¼Œè·³è¿‡ã€‚URLæ‰¾åˆ°æƒ…å†µ: '{original_url if original_url else 'æœªæ‰¾åˆ°'}'")
                                                continue

                                            source_info = f"æ¥æº: {vendor}/{subcategory}/{file_date_obj.isoformat()}/{filename}"
                                            
                                            article_data = {
                                                "raw_content": raw_content_for_llm,
                                                "vendor": vendor,
                                                "subcategory": subcategory,
                                                "original_filename": filename,
                                                "source_info_for_llm": source_info,
                                                "original_url": original_url, # å¯èƒ½ä¸ºç©º
                                                "date_published": file_date_obj.isoformat() # æ·»åŠ å‘å¸ƒæ—¥æœŸ
                                            }
                                            all_articles_data.append(article_data)
                                            logger.debug(f"å·²æ”¶é›†æ–‡ç« æ•°æ®: {filename} (URL: {original_url if original_url else 'æœªæ‰¾åˆ°'})")
                                        except Exception as e_read:
                                            logger.warning(f"æ— æ³•è¯»å–æˆ–å¤„ç†æ–‡ä»¶ {file_path}: {e_read}")
                            else:
                                logger.debug(f"æ–‡ä»¶ {filename} (åœ¨ {subcategory_path} ä¸­) ä¸ YYYY_MM_DD_* æ ¼å¼ä¸åŒ¹é…ã€‚")
                        except ValueError:
                            logger.debug(f"æ— æ³•ä»æ–‡ä»¶ {filename} (åœ¨ {subcategory_path} ä¸­) è§£ææ—¥æœŸã€‚æœŸæœ›æ ¼å¼ YYYY_MM_DD_*.mdã€‚")
                        except Exception as e_parse:
                            logger.warning(f"è§£ææ–‡ä»¶å {filename} (åœ¨ {subcategory_path} ä¸­) æ—¶å‡ºé”™: {e_parse}")
            except OSError as e_list_files:
                logger.warning(f"æ— æ³•åˆ—å‡º {subcategory_path} ä¸­çš„æ–‡ä»¶: {e_list_files}")
    
    if not all_articles_data:
        logger.info("å½“å‰å‘¨æœªæ‰¾åˆ°åŸå§‹æ–‡ç« ã€‚")
        return [] # è¿”å›ç©ºåˆ—è¡¨
    
    logger.info(f"æœ¬å‘¨æˆåŠŸæ”¶é›†åˆ° {len(all_articles_data)} ç¯‡åŸå§‹æ–‡ç« çš„æ•°æ®ã€‚")
    return all_articles_data # è¿”å›æ–‡ç« æ•°æ®åˆ—è¡¨

def generate_report_markdown_from_articles(
    articles_data: List[Dict[str, str]], 
    model_client: Any, 
    config: dict # ç”¨äºè·å–å†…éƒ¨é“¾æ¥åŸŸåç­‰é…ç½®
) -> Optional[str]:
    """
    å°†æ‰€æœ‰æ–‡ç« å†…å®¹åˆå¹¶åè¿›è¡Œå•æ¬¡LLMè°ƒç”¨ç”Ÿæˆå®Œæ•´æ‘˜è¦ï¼Œç„¶åæ›¿æ¢é“¾æ¥å¹¶æ ¼å¼åŒ–ä¸ºå‘¨æŠ¥Markdownã€‚
    """
    logger = logging.getLogger(__name__) # ä¸»å‡½æ•°æ—¥å¿—
    
    if not articles_data:
        logger.info("æ²¡æœ‰æ–‡ç« æ•°æ®ä¼ é€’ç»™ generate_report_markdown_from_articlesï¼Œä¸ç”ŸæˆæŠ¥å‘Šã€‚")
        return None

    # --- ä»é…ç½®ä¸­è·å–æŠ¥å‘Šç›¸å…³è®¾ç½® ---
    reporting_config = config.get("reporting", {})
    site_base_url = reporting_config.get("site_base_url", "http://cnetspy.site") 

    beautification_config = reporting_config.get("beautification", {})
    banner_url = beautification_config.get("banner_url", "")
    report_title_prefix = beautification_config.get("report_title_prefix", "ã€äº‘æŠ€æœ¯å‘¨æŠ¥ã€‘")
    intro_text = beautification_config.get("intro_text", "æ±‡é›†æœ¬å‘¨ä¸»è¦äº‘å‚å•†çš„æŠ€æœ¯äº§å“åŠ¨æ€ï¼ŒåŠ©æ‚¨å¿«é€ŸæŒæ¡æ ¸å¿ƒå˜åŒ–ã€‚")
    vendor_emojis = beautification_config.get("vendor_emojis", {"AWS": "ğŸŸ ", "AZURE": "ğŸ”µ", "GCP": "ğŸ”´", "DEFAULT": "â˜ï¸"})
    no_updates_text = beautification_config.get("no_updates_text", "æœ¬å‘¨æš‚æ— é‡è¦æ›´æ–°å†…å®¹ã€‚")
    footer_text = beautification_config.get("footer_text", "ç”±äº‘ç«äº‰æƒ…æŠ¥åˆ†æå¹³å°è‡ªåŠ¨æ±‡æ€»ã€‚")
    platform_link_text = beautification_config.get("platform_link_text", "å‰å¾€å¹³å°æŸ¥çœ‹æ›´å¤šè¯¦æƒ…")
    platform_url = beautification_config.get("platform_url", site_base_url) 

    # --- 1. å‡†å¤‡LLMçš„è¾“å…¥ï¼šåˆå¹¶æ‰€æœ‰æ–‡ç« å†…å®¹ --- 
    # æˆ‘ä»¬éœ€è¦åœ¨æç¤ºä¸­å‘ŠçŸ¥LLMï¼Œå®ƒå°†æ”¶åˆ°ä¸€ä¸ªåŒ…å«å¤šç¯‡æ–‡ç« çš„é›†åˆï¼Œ
    # å¹¶ä¸ºæ¯ç¯‡æ–‡ç« ç”Ÿæˆæ‘˜è¦ï¼Œç„¶åå°†æ‰€æœ‰æ‘˜è¦ç»„åˆæˆä¸€ä¸ªè¿è´¯çš„MarkdownæŠ¥å‘Šç‰‡æ®µã€‚
    # æç¤ºè¯ä¹Ÿéœ€è¦å¼ºè°ƒï¼ŒLLMç”Ÿæˆçš„é“¾æ¥åº”ä½¿ç”¨åŸæ–‡URLï¼Œæˆ‘ä»¬åç»­ä¼šå¤„ç†ã€‚

    llm_input_parts = []
    for article in articles_data:
        # ä¸ºäº†å¸®åŠ©åç»­URLæ›¿æ¢å’Œå†…å®¹æ ¡éªŒï¼Œå¯ä»¥åœ¨æ¯ç¯‡æ–‡ç« å‰ååŠ å…¥ç‰¹æ®Šæ ‡è®°ï¼Œæˆ–ç¡®ä¿LLMè¾“å‡ºåŒ…å«è¶³å¤Ÿä¿¡æ¯
        # ä¾‹å¦‚ï¼Œå¯ä»¥åœ¨source_info_for_llmä¸­åŒ…å«ä¸€ä¸ªå”¯ä¸€IDæˆ–æ–‡ä»¶å
        # æç¤ºè¯ä¹Ÿåº”æŒ‡å¯¼LLMåœ¨ç”Ÿæˆæ¯ä¸ªæ‘˜è¦æ—¶ï¼Œèƒ½å¤Ÿæ¸…æ™°åœ°æŒ‡æ˜æ¥æºæ–‡ç« 
        llm_input_parts.append(f"--- æ–‡ç« å¼€å§‹ ---\næ¥æºä¿¡æ¯: {article['source_info_for_llm']}\nåŸæ–‡URL: {article['original_url']}\næ–‡ç« åŸå§‹å†…å®¹:\n{article['raw_content']}\n--- æ–‡ç« ç»“æŸ ---\n\n") # æ–‡ç« é—´ç•™ç©ºè¡Œ
    
    full_llm_input_prompt = "\n".join(llm_input_parts)
    if not full_llm_input_prompt.strip():
        logger.warning("å‡†å¤‡çš„LLMè¾“å…¥å†…å®¹ä¸ºç©ºï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚")
        return None
    
    logger.info(f"å·²ä¸º {len(articles_data)} ç¯‡æ–‡ç« å‡†å¤‡å¥½åˆå¹¶çš„LLMè¾“å…¥ï¼Œæ€»é•¿åº¦çº¦: {len(full_llm_input_prompt)} chars")
    logger.debug(f"å‘é€ç»™LLMçš„åˆå¹¶å†…å®¹ (å‰500å­—ç¬¦é¢„è§ˆ):\n{full_llm_input_prompt[:500]}...")

    # --- 2. å•æ¬¡LLMè°ƒç”¨ --- 
    generated_report_content_raw = None
    try:
        generated_report_content_raw = model_client.predict(prompt=full_llm_input_prompt)
        if not isinstance(generated_report_content_raw, str) or not generated_report_content_raw.strip():
            logger.error("LLMè¿”å›çš„æŠ¥å‘Šå†…å®¹ä¸ºç©ºæˆ–æ— æ•ˆã€‚")
            # å³ä½¿LLMå¤±è´¥ï¼Œä¹Ÿå°è¯•ç”ŸæˆåŒ…å«æ¨¡æ¿ä¿¡æ¯çš„æŠ¥å‘Š
            generated_report_content_raw = beautification_config.get("dingtalk_no_updates_text", "æœ¬å‘¨å„å®¶äº‘å‚å•†åœ¨æ‰€ç›‘æ§çš„æŠ€æœ¯é¢†åŸŸå†…æš‚æ— é‡è¦æ›´æ–°å†…å®¹å…¬å¼€å‘å¸ƒã€‚") 
        else:
            logger.info("LLMæˆåŠŸè¿”å›æŠ¥å‘Šå†…å®¹ã€‚")
            logger.debug(f"LLMè¿”å›çš„åŸå§‹æŠ¥å‘Šå†…å®¹ (å‰500å­—ç¬¦é¢„è§ˆ):\n{generated_report_content_raw[:500]}...")
    except Exception as e:
        logger.error(f"è°ƒç”¨LLMç”Ÿæˆæ•´ä½“æŠ¥å‘Šæ—¶å‡ºé”™: {e}", exc_info=True)
        # å³ä½¿LLMå¤±è´¥ï¼Œä¹Ÿå°è¯•ç”ŸæˆåŒ…å«æ¨¡æ¿ä¿¡æ¯çš„æŠ¥å‘Š
        generated_report_content_raw = beautification_config.get("dingtalk_no_updates_text", "æœ¬å‘¨å„å®¶äº‘å‚å•†åœ¨æ‰€ç›‘æ§çš„æŠ€æœ¯é¢†åŸŸå†…æš‚æ— é‡è¦æ›´æ–°å†…å®¹å…¬å¼€å‘å¸ƒã€‚") 

    # --- 3. URLæ›¿æ¢å’Œå†…å®¹æ ¡éªŒ (å…³é”®ä¸”å¤æ‚çš„éƒ¨åˆ†) ---
    # ç°åœ¨çš„ generated_report_content_raw æ˜¯ä¸€ä¸ªå¤§çš„Markdownå­—ç¬¦ä¸²ï¼ŒåŒ…å«æ‰€æœ‰æ–‡ç« çš„æ‘˜è¦
    # æˆ‘ä»¬éœ€è¦éå†å®ƒï¼Œæ‰¾åˆ°æ‰€æœ‰å½¢å¦‚ ### [[å‚å•†] æ ‡é¢˜](åŸæ–‡URL) çš„é“¾æ¥ï¼Œ
    # å¹¶å°† åŸæ–‡URL æ›¿æ¢ä¸ºå¯¹åº”çš„ å†…éƒ¨URLã€‚
    # articles_data åˆ—è¡¨åœ¨è¿™é‡Œè‡³å…³é‡è¦ï¼Œæˆ‘ä»¬éœ€è¦ç”¨å®ƒæ¥æŸ¥æ‰¾æ¯ç¯‡æ–‡ç« çš„å…ƒæ•°æ®ä»¥æ„å»ºå†…éƒ¨é“¾æ¥ã€‚
    
    processed_report_content = generated_report_content_raw # åˆå§‹åŒ–
    
    # TODO: å®ç°æ›´å¥å£®çš„URLæ›¿æ¢é€»è¾‘ã€‚
    # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆçš„å ä½é€»è¾‘ï¼Œéœ€è¦ç”¨æ­£åˆ™è¡¨è¾¾å¼å’Œæ›´ç²¾ç¡®çš„åŒ¹é…æ¥å®Œå–„ã€‚
    # ç†æƒ³æƒ…å†µä¸‹ï¼ŒLLMçš„è¾“å‡ºåº”ä¸¥æ ¼éµå¾ªæ ¼å¼ï¼Œä¾¿äºè§£æã€‚
    # æˆ‘ä»¬éœ€è¦éå† articles_dataï¼Œå¯¹æ¯ç¯‡æ–‡ç« å°è¯•åœ¨ processed_report_content ä¸­æ‰¾åˆ°å…¶å¯¹åº”çš„æ‘˜è¦å’Œé“¾æ¥ã€‚

    temp_summaries_by_vendor = {} # ä¸´æ—¶æŒ‰å‚å•†å­˜æ”¾å¤„ç†åçš„æ‘˜è¦ç‰‡æ®µ
    # å‡è®¾LLMä¸ºæ¯ç¯‡æ–‡ç« éƒ½ç”Ÿæˆäº† ### [[å‚å•†] æ ‡é¢˜](åŸæ–‡URL) æ ¼å¼çš„æ‘˜è¦ï¼Œå¹¶ä¸”æŒ‰é¡ºåºæ’åˆ—
    # è¿™åªæ˜¯ä¸€ä¸ªéå¸¸ç²—ç•¥çš„è®¾æƒ³ï¼Œå®é™…LLMçš„è¾“å‡ºå¯èƒ½éœ€è¦æ›´å¤æ‚çš„è§£æ

    # ç¤ºä¾‹ï¼šå¦‚æœLLMä¸¥æ ¼æŒ‰é¡ºåºè¾“å‡ºäº†æ‘˜è¦ï¼Œå¹¶ä¸”æ¯ä¸ªæ‘˜è¦éƒ½æœ‰å”¯ä¸€å¯è¯†åˆ«çš„æ ‡é¢˜æˆ–åŸæ–‡URL
    # æˆ‘ä»¬å¯ä»¥å°è¯•è¿­ä»£ articles_dataï¼Œç„¶ååœ¨ generated_report_content_raw ä¸­å¯»æ‰¾å¹¶æ›¿æ¢
    # è¿™éƒ¨åˆ†éå¸¸ä¾èµ–LLMçš„è¾“å‡ºæ ¼å¼ï¼Œéœ€è¦æ ¹æ®å®é™…è¾“å‡ºæ¥è°ƒæ•´
    
    # éå†åŸå§‹æ–‡ç« æ•°æ®ï¼Œå°è¯•åœ¨LLMçš„è¾“å‡ºä¸­æ‰¾åˆ°å¹¶æ›¿æ¢é“¾æ¥
    # è¿™éœ€è¦LLMè¾“å‡ºçš„æ‘˜è¦æ ‡é¢˜æˆ–å†…å®¹ä¸åŸæ–‡æœ‰è¶³å¤Ÿé«˜çš„ç›¸ä¼¼åº¦ï¼Œæˆ–è€…LLMèƒ½æŒ‰æˆ‘ä»¬æä¾›çš„åŸæ–‡URLè¾“å‡º
    if generated_report_content_raw and generated_report_content_raw != no_updates_text: # ä»…å½“LLMæœ‰æœ‰æ•ˆè¾“å‡ºæ—¶å°è¯•æ›¿æ¢
        processed_llm_output = generated_report_content_raw # ä½¿ç”¨ä¸€ä¸ªæ–°å˜é‡è¿›è¡Œæ“ä½œï¼Œä¿ç•™åŸå§‹LLMè¾“å‡ºä»¥ä¾›è°ƒè¯•
        
        logger.info("å¼€å§‹åœ¨LLMç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹ä¸­æ›¿æ¢åŸæ–‡URLä¸ºå†…éƒ¨é“¾æ¥...")
        replacement_attempts = 0
        successful_replacements = 0

        for article_info in articles_data:
            original_url = article_info.get("original_url")
            source_info_log = article_info.get('source_info_for_llm', article_info.get('original_filename', 'æœªçŸ¥æ–‡ç« '))

            logger.debug(
                f"URL_REPLACE: æ­£åœ¨å¤„ç†æ–‡ç«  '{source_info_log}'. "
                f"ä»å…ƒæ•°æ®è·å–çš„ Original URL: '{original_url}' (ç±»å‹: {type(original_url)})"
            )

            if not original_url or not isinstance(original_url, str) or not original_url.strip().startswith("http"):
                logger.warning(
                    f"URL_REPLACE_SKIP: è·³è¿‡æ–‡ç«  '{source_info_log}' çš„URLæ›¿æ¢. "
                    f"åŸå› : å…ƒæ•°æ®ä¸­çš„ Original URL æ— æ•ˆæˆ–ç¼ºå¤± (å€¼: '{original_url}')."
                )
                continue

            replacement_attempts += 1
            
            # æ„å»ºå†…éƒ¨é“¾æ¥
            vendor = article_info.get("vendor", "unknown_vendor")
            subcategory = article_info.get("subcategory", "unknown_subcategory")
            original_filename_from_data = article_info.get("original_filename")

            if not original_filename_from_data:
                logger.warning(f"URL_REPLACE_WARNING: Article for source '{source_info_log}' is missing 'original_filename' in its data. Cannot build internal URL.")
                continue
            
            # ç›´æ¥ä½¿ç”¨ original_filename_from_dataï¼Œå› ä¸ºå®ƒåº”è¯¥åŒ…å« .md åç¼€
            internal_link_path = f"analysis/document/{vendor}/{subcategory}/{original_filename_from_data}"
            internal_link_full = f"{site_base_url}/{internal_link_path}" 
            
            pattern_to_find = f"]({original_url})"
            replacement_pattern = f"]({internal_link_full})"

            logger.debug(
                f"URL_REPLACE_ATTEMPT: æ–‡ç«  '{source_info_log}'. "
                f"æŸ¥æ‰¾æ¨¡å¼: '{pattern_to_find}'. "
                f"æ›¿æ¢ä¸º: '{replacement_pattern}'."
            )
            
            if pattern_to_find in processed_llm_output:
                processed_llm_output = processed_llm_output.replace(pattern_to_find, replacement_pattern)
                successful_replacements += 1
                logger.info(
                    f"URL_REPLACE_SUCCESS: æˆåŠŸæ›¿æ¢æ–‡ç«  '{source_info_log}' çš„URL. "
                    f"'{original_url}' -> '{internal_link_full}'."
                )
            else:
                logger.warning(
                    f"URL_REPLACE_FAILED: æœªèƒ½æ›¿æ¢æ–‡ç«  '{source_info_log}' çš„URL. "
                    f"æŸ¥æ‰¾æ¨¡å¼ '{pattern_to_find}' æœªåœ¨LLMå½“å‰å¤„ç†çš„è¾“å‡ºä¸­æ‰¾åˆ°. "
                    f"Original URL from metadata: '{original_url}'."
                )
                if original_url in generated_report_content_raw:
                    logger.warning(
                        f"URL_REPLACE_HINT: æ¨¡å¼ '{pattern_to_find}' æœªæ‰¾åˆ°, "
                        f"ä½†åŸå§‹URL '{original_url}' æœ¬èº«å­˜åœ¨äºLLMæœªç»ä¿®æ”¹çš„è¾“å‡ºä¸­. "
                        f"è¿™å¯èƒ½æ„å‘³ç€LLMè¾“å‡ºçš„é“¾æ¥æ ¼å¼ä¸æœŸæœ›çš„ '](URL)' ä¸å®Œå…¨åŒ¹é…."
                    )
                    try:
                        search_term_for_context = original_url.lower().rstrip('/')
                        
                        idx = -1
                        variations_to_try = [
                            original_url, 
                            original_url.rstrip('/'), 
                            original_url.lower(), 
                            search_term_for_context 
                        ]
                        found_variation_for_context = None
                        for variation in variations_to_try:
                            temp_idx = generated_report_content_raw.find(variation)
                            if temp_idx != -1:
                                idx = temp_idx
                                found_variation_for_context = variation
                                logger.info(f"URL_REPLACE_CONTEXT: Found variation '{variation}' of original URL in raw LLM output at char index {idx}.")
                                break
                        
                        if idx != -1:
                            context_start = max(0, idx - 70)
                            context_end = min(len(generated_report_content_raw), idx + len(found_variation_for_context) + 70)
                            context_snippet = generated_report_content_raw[context_start:context_end]
                            # Safely construct the log message
                            # Using repr() for context_snippet to avoid issues with special characters in it
                            log_message_part1 = f"URL_REPLACE_CONTEXT: LLMåŸå§‹è¾“å‡ºä¸­ä¸ '{original_url}' (æ‰¾åˆ°çš„å˜ä½“: '{found_variation_for_context}') ç›¸å…³çš„ä¸Šä¸‹æ–‡ç‰‡æ®µ "
                            log_message_part2 = f"(ä½äºå­—ç¬¦ç´¢å¼• {idx} é™„è¿‘):"
                            log_message_part3 = f"---...{repr(context_snippet)}...---"
                            logger.warning(log_message_part1 + log_message_part2 + log_message_part3)
                        else:
                            logger.warning(
                                f"URL_REPLACE_CONTEXT: å³ä½¿æ˜¯åŸå§‹URL '{original_url}' (æˆ–å…¶å˜ä½“) "
                                f"ä¹Ÿæœªèƒ½åœ¨æœªç»ä¿®æ”¹çš„LLMè¾“å‡ºä¸­ç›´æ¥å®šä½åˆ°. LLMå¯èƒ½å®Œå…¨æ”¹å˜äº†å®ƒæˆ–æ²¡æœ‰åŒ…å«å®ƒ."
                            )
                    except Exception as e_context:
                        logger.error(f"URL_REPLACE_CONTEXT: æå–ä¸Šä¸‹æ–‡æ—¶å‘ç”Ÿé”™è¯¯: {e_context}", exc_info=True)
                else:
                     logger.warning(
                        f"URL_REPLACE_HINT: åŸå§‹URL '{original_url}' æœ¬èº«ä¹Ÿæœªåœ¨æœªç»ä¿®æ”¹çš„LLMè¾“å‡ºä¸­æ‰¾åˆ°. "
                        f"è¯·æ£€æŸ¥LLMæ˜¯å¦æŒ‰é¢„æœŸåŒ…å«äº†è¯¥URL."
                    )
        
        logger.info(f"URLæ›¿æ¢å®Œæˆã€‚å°è¯•æ¬¡æ•°: {replacement_attempts}, æˆåŠŸæ¬¡æ•°: {successful_replacements}.")
        processed_report_content = processed_llm_output 
    else:
        logger.info("LLMæœªè¿”å›æœ‰æ•ˆå†…å®¹æˆ–æ— æ–‡ç« æ•°æ®ï¼Œè·³è¿‡URLæ›¿æ¢ã€‚")
        processed_report_content = generated_report_content_raw

    # --- 4. ç»„åˆæœ€ç»ˆçš„MarkdownæŠ¥å‘Š --- 
    final_markdown_parts = []
    total_articles_processed_successfully = len(articles_data) if processed_report_content and processed_report_content != no_updates_text else 0
    
    if banner_url:
        final_markdown_parts.append(f"![]({banner_url})\n")

    today_date_obj = datetime.date.today()
    start_of_week = today_date_obj - datetime.timedelta(days=today_date_obj.weekday())
    report_date_range_str = f"{start_of_week.strftime('%Yå¹´%mæœˆ%dæ—¥')} - {today_date_obj.strftime('%Yå¹´%mæœˆ%dæ—¥')}"
    
    report_main_title = f"{report_title_prefix} {report_date_range_str} ç«äº‰åŠ¨æ€é€Ÿè§ˆ"
    final_markdown_parts.append(f"# {report_main_title}\n")
    
    final_markdown_parts.append(f"{intro_text}\n")

    # ç›´æ¥æ·»åŠ LLMå¤„ç†è¿‡çš„å†…å®¹
    # å‡è®¾LLMçš„è¾“å‡ºå·²ç»æ˜¯æŒ‰å‚å•†åˆ†ç»„çš„æˆ–è€…æˆ‘ä»¬æ¥å—LLMçš„æ’åº
    if processed_report_content and processed_report_content != no_updates_text:
        final_markdown_parts.append(processed_report_content)
        # ç¡®ä¿LLMè¾“å‡ºåæœ‰ä¸€ä¸ªæ¢è¡Œï¼Œä»¥ä¾¿å’Œé¡µè„šåˆ†éš”
        if not processed_report_content.endswith('\n'):
            final_markdown_parts.append("\n")
    else: # LLMæ²¡æœ‰æœ‰æ•ˆè¾“å‡ºæˆ–è€…è¿”å›çš„æ˜¯æ— æ›´æ–°æ–‡æœ¬
        final_markdown_parts.append(no_updates_text + "\n")
        total_articles_processed_successfully = 0 # ç¡®è®¤è®¡æ•°

    if platform_url and platform_link_text:
        final_markdown_parts.append(f"{footer_text} [{platform_link_text}]({platform_url})")
    else:
        final_markdown_parts.append(footer_text)
        
    logger.info(f"æŠ¥å‘ŠMarkdownå†…å®¹å·²ç”Ÿæˆ (å¤„ç†äº†çº¦ {total_articles_processed_successfully} ç¯‡æ–‡ç« çš„æ‘˜è¦)ã€‚")
    return "\n".join(final_markdown_parts)

def main():
    # --- å‚æ•°è§£æ --- 
    parser = argparse.ArgumentParser(description="ç”Ÿæˆæ¯å‘¨ç«å“åŠ¨æ€æ‘˜è¦æŠ¥å‘Šã€‚")
    parser.add_argument(
        '--loglevel',
        default='INFO', # å¦‚æœæœªæŒ‡å®šï¼Œé»˜è®¤ä¸º INFO
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        help='è®¾ç½®æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)'
    )
    args = parser.parse_args()

    # å°†å­—ç¬¦ä¸²æ—¥å¿—çº§åˆ«è½¬æ¢ä¸º loggingå¸¸é‡
    numeric_log_level = getattr(logging, args.loglevel.upper(), None)
    if not isinstance(numeric_log_level, int):
        raise ValueError(f'æ— æ•ˆçš„æ—¥å¿—çº§åˆ«: {args.loglevel}')

    # é¦–å…ˆè®¾ç½®æ—¥å¿— - è¿™å¯èƒ½ä¼šä»é…ç½®ä¸­è®¾ç½®ä¸€ä¸ªé»˜è®¤çº§åˆ«
    setup_colored_logging() 
    
    # è·å–æ ¹æ—¥å¿—è®°å½•å™¨å¹¶æ ¹æ®å‘½ä»¤è¡Œå‚æ•°è®¾ç½®å…¶çº§åˆ«
    # è¿™å°†å½±å“æ‰€æœ‰æ—¥å¿—è®°å½•å™¨ï¼Œé™¤éå®ƒä»¬æœ‰è‡ªå·±è®¾ç½®çš„æ›´ä½çº§åˆ«ã€‚
    # æ³¨æ„: å¦‚æœ setup_colored_logging é…ç½®äº†å…·æœ‰ç‰¹å®šçº§åˆ«çš„å¤„ç†å™¨ï¼Œ
    # è¿™äº›å¤„ç†å™¨çš„çº§åˆ«å¯èƒ½ä¹Ÿéœ€è¦è°ƒæ•´ï¼Œæˆ–è€…æ­¤æ ¹çº§åˆ«è®¾ç½®å¯èƒ½å·²è¶³å¤Ÿã€‚
    logging.getLogger().setLevel(numeric_log_level)
    # å¦‚æœæƒ³æ›´ç²¾ç¡®åœ°è®¾ç½®ï¼Œå¯ä»¥ä¸ºç‰¹å®šçš„æ—¥å¿—è®°å½•å™¨è®¾ç½®çº§åˆ«:
    # logging.getLogger('__main__').setLevel(numeric_log_level)
    # logging.getLogger('src.utils').setLevel(numeric_log_level) # å…¶ä»–æ¨¡å—ç¤ºä¾‹

    logger = logging.getLogger(__name__) # åœ¨çº§åˆ«è®¾ç½®åè·å–logger

    logger.info(f"æ—¥å¿—çº§åˆ«å·²è®¾ç½®ä¸º: {args.loglevel}") 
    logger.info("å¼€å§‹ç”Ÿæˆæ¯å‘¨ç«å“åŠ¨æ€æ‘˜è¦è„šæœ¬ (å•æ¬¡LLMè°ƒç”¨æ¨¡å¼)ã€‚")

    try:
        config_full = get_config() 
        if not config_full:
            logger.error("ä¸¥é‡: é…ç½®åŠ è½½å¤±è´¥ã€‚`get_config()` è¿”å› Noneæˆ–ä¸ºç©ºã€‚è¯·ç¡®ä¿ config.yaml (æˆ–ç­‰æ•ˆæ–‡ä»¶) æœ‰æ•ˆä¸”å¯è®¿é—®ã€‚")
            return
        logger.info("é…ç½®åŠ è½½æˆåŠŸã€‚")
    except FileNotFoundError:
        logger.error("ä¸¥é‡: é…ç½®æ–‡ä»¶ (ä¾‹å¦‚ config.yaml) æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿å®ƒå­˜åœ¨äºé¡¹ç›®æ ¹ç›®å½•æˆ– config_loader.py çš„ç›¸å…³è·¯å¾„ä¸­ã€‚")
        return
    except Exception as e:
        logger.error(f"ä¸¥é‡: åŠ è½½é…ç½®æ—¶å‡ºé”™: {e}", exc_info=True)
        return

    # --- AIåˆ†æå™¨ (ModelManager) åˆå§‹åŒ– ---
    ai_config_params = config_full.get("ai_analyzer")
    if not ai_config_params:
        logger.error("ä¸¥é‡: é…ç½®ä¸­æœªæ‰¾åˆ° 'ai_analyzer' éƒ¨åˆ†ã€‚ModelManager éœ€è¦æ­¤éƒ¨åˆ†é…ç½®ã€‚")
        return
    
    try:
        model_manager = ModelManager(ai_config=ai_config_params)
        logger.info("ModelManager åˆå§‹åŒ–æˆåŠŸã€‚")
    except Exception as e:
        logger.error(f"ä¸¥é‡: åˆå§‹åŒ– ModelManager å¤±è´¥: {e}", exc_info=True)
        return

    # --- åŠ è½½æç¤ºæ¨¡æ¿ (ç”¨ä½œLLMå®¢æˆ·ç«¯çš„ç³»ç»Ÿæç¤º) ---
    # æ­¤æç¤ºå®šä¹‰äº†LLMå®¢æˆ·ç«¯å®ä¾‹çš„ä»»åŠ¡/è§’è‰²ã€‚
    prompt_file_path_config = config_full.get("prompt_paths", {})
    
    # ä» reporting é…ç½®ä¸­è·å–æç¤ºé”®
    reporting_config_main = config_full.get("reporting", {}) # åœ¨mainå‡½æ•°ä½œç”¨åŸŸä¹Ÿè·å–reporting_config
    weekly_update_prompt_key = reporting_config_main.get("weekly_update_prompt_key", "weekly_updates")
    
    prompt_file_path = prompt_file_path_config.get(weekly_update_prompt_key, f"prompt/{weekly_update_prompt_key}.txt")

    try:
        system_prompt_for_weekly_report = load_prompt_template(prompt_file_path)
        logger.info(f"æˆåŠŸä» {prompt_file_path} åŠ è½½å‘¨æŠ¥çš„ç³»ç»Ÿæç¤ºæ¨¡æ¿ã€‚")
    except Exception: # load_prompt_template å·²è®°å½•å…·ä½“é”™è¯¯
        logger.error(f"ä¸¥é‡: åŠ è½½ç³»ç»Ÿæç¤ºæ¨¡æ¿å¤±è´¥ã€‚è¯·ç¡®ä¿ '{prompt_file_path}' å­˜åœ¨ä¸”å¯è¯»ã€‚")
        return

    # --- ä» ModelManager è·å–é…ç½®å¥½çš„ LLM å®¢æˆ·ç«¯ ---
    # ç¡®å®šæ­¤æŠ¥å‘Šä½¿ç”¨å“ªä¸ªæ¨¡å‹é…ç½®
    # å¦‚æœ reporting é…ç½®ä¸­æœªæŒ‡å®šï¼Œåˆ™é»˜è®¤ä¸º ai_analyzer é…ç½®ä¸­çš„ active_model_profile
    default_profile_name = model_manager.active_model_profile_name 
    report_model_profile_name = reporting_config_main.get("weekly_summary_model_profile", default_profile_name)

    if not report_model_profile_name:
        logger.error("ä¸¥é‡: æ— æ³•ç¡®å®šå‘¨æŠ¥ä½¿ç”¨çš„æ¨¡å‹é…ç½®ã€‚é…ç½®ä¸­ 'reporting.weekly_summary_model_profile' å’Œ 'ai_analyzer.active_model_profile' å‡æœªè®¾ç½®ã€‚")
        return

    try:
        logger.info(f"å°è¯•è·å–LLMå®¢æˆ·ç«¯ï¼Œé…ç½®åç§°: '{report_model_profile_name}'ï¼Œä½¿ç”¨å·²åŠ è½½çš„ç³»ç»Ÿæç¤ºã€‚")
        # system_prompt_for_weekly_report (æ¥è‡ª weekly_updates.txt) å°†é…ç½®LLMå®¢æˆ·ç«¯çš„è¡Œä¸ºã€‚
        llm_client_for_report = model_manager.get_model_client(
            system_prompt_text=system_prompt_for_weekly_report,
            model_profile_name=report_model_profile_name
        )
        logger.info(f"æˆåŠŸè·å–é…ç½®ä¸º '{report_model_profile_name}' çš„LLMå®¢æˆ·ç«¯ã€‚")
    except AIAnalyzerError as e:
        logger.error(f"ä¸¥é‡: ä» ModelManager è·å–LLMå®¢æˆ·ç«¯å¤±è´¥ (AIAnalyzerError): {e}", exc_info=True)
        return
    except ValueError as e: # æ•è· ModelManager çš„ ValueError (ä¾‹å¦‚ï¼Œé…ç½®æœªæ‰¾åˆ°)
        logger.error(f"ä¸¥é‡: ä» ModelManager è·å–LLMå®¢æˆ·ç«¯å¤±è´¥ (ValueError): {e}", exc_info=True)
        return
    except Exception as e: # æ•è·ä»»ä½•å…¶ä»–æ„å¤–é”™è¯¯
        logger.error(f"ä¸¥é‡: ä» ModelManager è·å–LLMå®¢æˆ·ç«¯æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        return
        
    # --- è·å–åŸå§‹å†…å®¹ --- 
    logger.info("æ­£åœ¨å°è¯•è·å–æœ¬å‘¨æ‰€æœ‰æ–‡ç« çš„è¯¦ç»†æ•°æ®...")
    # all_articles_raw_content åŸæ¥æ˜¯å­—ç¬¦ä¸²ï¼Œç°åœ¨æ˜¯ List[Dict[str,str]]
    articles_data_list = get_all_raw_content_for_week(config_full)

    if not articles_data_list: # ä¿®æ”¹äº†å˜é‡åå’Œåˆ¤æ–­
        logger.info("æœªæ‰¾åˆ°æœ¬å‘¨åŸå§‹æ–‡ç« æ•°æ®ã€‚å°†ç”Ÿæˆä¸€ä¸ªç©ºçš„æŠ¥å‘Šæ¡†æ¶ã€‚")
        # å³ä½¿æ²¡æœ‰æ–‡ç« ï¼Œä¹Ÿå°è¯•ç”Ÿæˆä¸€ä¸ªç©ºçš„æŠ¥å‘Šæ¡†æ¶ï¼Œæˆ–è€…åªåŒ…å«æ— æ›´æ–°æç¤ºçš„æŠ¥å‘Š
        # generate_report_markdown_from_articles å†…éƒ¨ä¼šå¤„ç† articles_data_list ä¸ºç©ºçš„æƒ…å†µ

    weekly_summary_md = generate_report_markdown_from_articles(
        articles_data=articles_data_list, # ä¼ é€’æ–‡ç« æ•°æ®åˆ—è¡¨
        model_client=llm_client_for_report,
        config=config_full # ä¼ é€’å®Œæ•´çš„é…ç½®å¯¹è±¡
    )
    
    if not weekly_summary_md:
        logger.info("æœªç”Ÿæˆå‘¨æŠ¥Markdownå†…å®¹ã€‚å°†ä¸åˆ›å»ºè¾“å‡ºæ–‡ä»¶ã€‚")
        return

    # --- è¾“å‡º Markdown ---
    final_markdown_content = weekly_summary_md 

    output_dir_config = config_full.get("output_paths", {})
    output_dir = output_dir_config.get("reports_dir", "data/reports")
    
    try:
        os.makedirs(output_dir, exist_ok=True)
        logger.info(f"å·²ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨: {output_dir}")
    except OSError as e:
        logger.error(f"æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½• {output_dir}: {e}")
        return 

    today_date_obj = datetime.date.today()
    start_of_week = today_date_obj - datetime.timedelta(days=today_date_obj.weekday()) # æœ¬å‘¨ä¸€
    
    report_title_range = f"{start_of_week.strftime('%Y-%m-%d')} åˆ° {today_date_obj.strftime('%Y-%m-%d')}"
    output_filename_date_range = f"{start_of_week.strftime('%Y-%m-%d')}_to_{today_date_obj.strftime('%Y-%m-%d')}"
    output_filename = f"weekly_competitor_summary_{output_filename_date_range}.md"
    output_filepath = os.path.join(output_dir, output_filename)

    try:
        with open(output_filepath, 'w', encoding='utf-8') as f:
            f.write(final_markdown_content) # final_markdown_content ç°åœ¨æ˜¯å®Œæ•´çš„æŠ¥å‘Š
        logger.info(f"æˆåŠŸå°†å‘¨æŠ¥å†™å…¥: {output_filepath}")
    except IOError as e:
        logger.error(f"å†™å…¥æ‘˜è¦æ–‡ä»¶ {output_filepath} æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    main() 