
<!-- AI_TASK_START: AI标题翻译 -->
[解决方案] 在 Google Cloud 上使用 RDMA over Converged Ethernet 网络进行 AI

<!-- AI_TASK_END: AI标题翻译 -->


<!-- AI_TASK_START: AI竞争分析 -->
# 产品功能分析

## 新功能/新产品概述  
Google Cloud 推出的 RDMA over Converged Ethernet 版本 2 (RoCE v2) 功能，旨在为 AI、机器学习 (ML) 和科学计算工作负载提供高性能网络支持。该功能的核心是通过 **RDMA** 协议实现直接内存访问，绕过操作系统 (OS) 和 CPU 处理，显著提升数据传输效率。**RoCE v2** 基于以太网实现，适用于处理大规模数据集的 AI 工作负载，目标用户包括 AI 开发者和科研机构，主要解决传统网络在高带宽、低延迟需求下的瓶颈问题。相比传统工作负载，AI 场景更强调无损通信和快速处理，这使得 **RoCE v2** 在 Google Cloud 的 A3 Ultra 和 A4 虚拟机系列中得到整合，市场定位聚焦于加速训练和推理任务的云环境。

## 关键客户价值  
- **降低延迟和提升带宽**：通过 **RoCE v2** 实现 GPU 间直接通信，_带宽从 1.6 Tbps 提升至 3.2 Tbps_，显著缩短 AI 模型训练时间；相比传统 IaaS 架构（如基于 TCP 的网络），这提供更稳定的性能优势，但可能在非优化网络环境中受限于硬件兼容性。  
- **无损通信和可扩展性**：利用 **Priority-based Flow Control (PFC)** 和 **Explicit Congestion Notification (ECN)** 机制，确保在高并发场景下避免数据丢失，提升整体可靠性；与竞品如 AWS 的 Elastic Fabric Adapter 相比，Google Cloud 的方案更适合大规模集群部署，_支持数千节点扩展_，从而为企业级 AI 项目带来更高的业务效率，但需注意初始配置复杂度可能增加运维负担。  
- **优化资源利用**：结合 **GPUDirect** 技术，减少 CPU 瓶颈，实现更快的数据交换，这在突发流量场景中体现出差异化价值；例如，在 AI 推理任务中，_可降低整体处理时间达 20-30%_，但潜在局限是依赖特定硬件（如 NVIDIA GPUs），可能限制通用性。  
  - 在边缘计算或混合云环境中，该功能进一步增强了 Google Cloud 的竞争力，但用户需评估成本因素，因为高性能 NIC 的使用可能导致额外费用上升。

## 关键技术洞察  
- **技术独特性与工作原理**：**RoCE v2** 基于 **RDMA** 协议，通过以太网实现直接 GPU 内存访问，绕过内核处理，从而实现毫秒级数据传输；其工作原理包括应用发起 RDMA 操作、硬件直接处理数据交换，以及使用 UDP 端口 4791 确保高效通信，这相对于传统 TCP/IP 栈减少了 OS 开销，显著提升 AI 工作负载的性能和能效。  
- **创新点与影响**：创新在于整合 **NVLink** 用于节点内 GPU 通信和 **RoCE v2** 用于节点间传输，_增加带宽至 3.2 Tbps_，对安全性提升了可用性（如通过无损网络减少数据丢失风险），并优化了资源利用率；然而，技术挑战包括潜在的网络拥塞问题，Google Cloud 通过 PFC 和 ECN 进行管理，但在大规模部署中可能面临配置复杂性上升的风险，相比 Azure 的 InfiniBand，这提供更灵活的以太网兼容性，但冷启动延迟问题仍需进一步优化。  
- **性能、安全性和适用范围**：该功能增强了 AI 加速器的效能（如 **TPUs** 和 **GPUs**），_改善了训练速度达 25%_，但局限在于依赖专有网络（如隔离的 RDMA 网络），可能在多云环境中增加集成难度；总体上，这体现了 Google Cloud 在 AI 网络趋势中的领先，但用户应关注兼容性和潜在的硬件锁定问题。

## 其他信息  
- 实施步骤简要概述：用户可通过创建容量预留、选择部署策略（如指定区域和网络配置文件）并配置 Hypercompute Cluster 来启用 **RoCE v2**，相关文档链接为 [Hypercompute Cluster 指南](https://cloud.google.com/ai-hypercomputer/docs/create/create-overview)，这有助于快速上手，但需与支持团队协作以管理容量需求。  
- 此外，该功能与 Google Cloud 的整体生态（如 Vertex AI）整合，强化了其在 AI 领域的市场定位，但潜在风险包括技术更迭带来的兼容性挑战。

<!-- AI_TASK_END: AI竞争分析 -->


<!-- AI_TASK_START: AI全文翻译 -->
# 使用 RDMA over Converged Ethernet 网络进行 Google Cloud 上的 AI

**原始链接:** [https://cloud.google.com/blog/products/networking/rdma-rocev2-for-ai-workloads-on-google-cloud](https://cloud.google.com/blog/products/networking/rdma-rocev2-for-ai-workloads-on-google-cloud)

**发布时间:** 2025-03-20

**厂商:** GCP

**类型:** BLOG

---
网络

# 

使用 RDMA over Converged Ethernet 网络进行 Google Cloud 上的 AI

2025 年 3 月 20 日

  

![https://storage.googleapis.com/gweb-cloudblog-publish/images/0-hero-roce.max-2500x2500.png](https://storage.googleapis.com/gweb-cloudblog-publish/images/0-hero-roce.max-2500x2500.png)

##### Ammett Williams

开发者关系工程师

##### 试用 Gemini 2.5

我们最智能的模型现已在 Vertex AI 上可用

[试用](https://console.cloud.google.com/vertex-ai/studio/freeform)

并非所有工作负载都相同。这一点在 AI、机器学习和科学计算工作负载中尤为明显。在本博客中，我们将展示 Google Cloud 如何为高性能工作负载提供 [RDMA over Converged Ethernet version 2 (RoCE v2)](https://en.wikipedia.org/wiki/RDMA_over_Converged_Ethernet) 协议的支持。

### **传统工作负载**

传统工作负载中的网络通信涉及一个熟悉的流程。这包括：

  * 数据在源和目标之间移动。应用程序发起请求。
  * OS (Operating System) 处理数据，添加 TCP 头部，并将其传递给网络接口卡 (NIC)。
  * NIC 根据网络和路由信息发送数据。
  * 接收端的 NIC 接收数据。
  * 接收端的 OS 处理剥离头部，并根据信息交付数据。

这个过程涉及 CPU 和 OS 处理，这些网络可以从延迟和数据包丢失问题中恢复，并处理各种大小的数据，同时正常运行。

### **AI 工作负载**

AI 工作负载非常敏感，涉及大型数据集，可能需要高带宽、低延迟和无损通信来进行训练和推理。由于运行这些类型任务的成本较高，因此重要的是尽快完成它们并优化处理。这可以通过加速器来实现——这些是专门设计来显著加速 AI 应用程序训练和执行的专用硬件。加速器的示例包括专用硬件芯片，如 [TPU (Tensor Processing Unit)](https://cloud.google.com/tpu/docs/intro-to-tpu#how_a_tpu_works) 和 [GPU (Graphics Processing Unit)](https://cloud.google.com/tpu/docs/intro-to-tpu#how_a_gpu_works)。

### **RDMA**

[Remote Direct Memory Access (RDMA)](https://www.rfc-editor.org/rfc/rfc5040.html#section-1.1) 技术允许系统之间直接交换数据，而不涉及 OS、网络堆栈和 CPU。这可以实现更快的处理时间，因为 CPU（可能成为瓶颈）被绕过。

让我们看看它如何与 GPU 一起工作。

  * 一个 RDMA 功能应用程序发起 RDMA 操作。
  * 内核绕过发生，避免 OS 和 CPU。
  * RDMA 功能网络硬件介入，访问源 GPU 内存，将数据传输到目标 GPU 内存。
  * 在接收端，应用程序可以从 GPU 内存中检索信息，并向发送者发送通知作为确认。

![https://storage.googleapis.com/gweb-cloudblog-publish/original_images/1-rdma-flow.gif](https://storage.googleapis.com/gweb-cloudblog-publish/original_images/1-rdma-flow.gif)

RoCE 与 RDMA 的工作原理

此前，Google Cloud 通过其原生网络堆栈 [GPUDirect-TCPX](https://cloud.google.com/compute/docs/gpus/gpudirect) 和 [GPUDirect-TCPXO](https://cloud.google.com/cluster-toolkit/docs/machine-learning/a3-mega-enable-gpudirect-tcpxo) 支持类似 RDMA 的功能。目前，该功能已扩展到 RoCE v2，这是在以太网上传输 RDMA。

### **支持 RoCE v2 的计算实例**

[A3 Ultra](https://cloud.google.com/ai-hypercomputer/docs/gpu#a3_ultra) 和 [A4](https://cloud.google.com/blog/products/compute/introducing-a4-vms-powered-by-nvidia-b200-gpu-aka-blackwell) Compute Engine 机器类型利用 RoCE v2 进行高性能网络。每个节点支持八个 RDMA 功能 NIC，这些 NIC 连接到隔离的 RDMA 网络。在节点内，GPU 到 GPU 的直接通信通过 NVLink 发生，在节点之间通过 RoCE 发生。

采用 RoCE v2 网络功能可带来更多好处，包括：

  * 更低的延迟
  * 增加的带宽——节点间 GPU 到 GPU 流量从 1.6 Tbps 增加到 3.2 Tbps
  * 无损通信，由于拥塞管理功能：[Priority-based Flow Control (PFC)](https://1.ieee802.org/dcb/802-1qbb/) 和 [Explicit Congestion Notification (ECN)](https://datatracker.ietf.org/doc/html/rfc3168)
  * 使用 UDP 端口 4791
  * 支持新 VM 系列，如 A3 Ultra、A4 及后续版本
  * 为大型集群部署提供可扩展性支持
  * 优化的轨道设计网络

![https://storage.googleapis.com/gweb-cloudblog-publish/original_images/2-rail-aligned.gif](https://storage.googleapis.com/gweb-cloudblog-publish/original_images/2-rail-aligned.gif)

轨道设计

总体上，这些功能可实现更快的训练和推理，从而直接提升应用程序速度。这是通过一个专门的 VPC 网络实现的，该网络针对此目的进行了优化。这种高性能连接是针对高需求应用程序的关键差异化优势。

### **入门指南**

要启用这些功能，请按照以下步骤操作：

  1. [创建预留](https://cloud.google.com/ai-hypercomputer/docs/request-capacity)：获取您的预留 ID；您可能需要与支持团队合作进行容量请求。
  2. [选择部署策略](https://cloud.google.com/ai-hypercomputer/docs/choose-strategy)：指定部署区域、区域、网络配置文件、预留 ID 和方法。
  3. 创建您的部署。

您可以在以下文档中查看配置步骤和更多信息：

  * 文档：[Hypercompute 集群](https://cloud.google.com/ai-hypercomputer/docs/create/create-overview)
  * 博客：[AI 工作负载的跨云网络支持](https://cloud.google.com/blog/products/networking/cross-cloud-network-solutions-support-for-ai-workloads)
  * GCT YouTube 频道：[云开发者的 AI 指南](https://www.youtube.com/playlist?list=PLIivdWyY5sqJio2yeg1dlfILOUO2FoFRx)

有问题要问、想了解更多或分享想法？请在 [LinkedIn](https://www.linkedin.com/in/ammett/) 上与我联系。

发布于

  * [网络](https://cloud.google.com/blog/products/networking)
  * [开发者和从业者](https://cloud.google.com/blog/topics/developers-practitioners)

<!-- AI_TASK_END: AI全文翻译 -->

