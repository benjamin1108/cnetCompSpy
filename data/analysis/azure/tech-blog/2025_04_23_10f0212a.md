
<!-- AI_TASK_START: AI标题翻译 -->
[新产品/新功能] 使用Cilium驱动的Azure CNI实现高规模Kubernetes网络

<!-- AI_TASK_END: AI标题翻译 -->


<!-- AI_TASK_START: AI竞争分析 -->
# 解决方案分析

## 解决方案概述  
Azure CNI Powered by Cilium 是一个针对 Kubernetes 集群网络的解决方案，旨在通过 **eBPF**（扩展 Berkeley Packet Filter）技术提升网络性能、安全性和可观察性。该方案解决的问题包括高效的 pod 网络通信、减少控制平面负载以及增强网络安全，适用于大规模 Kubernetes 部署场景。背景在于 Kubernetes 用户面临的高延迟、网络瓶颈和安全管理挑战，尤其在分布式应用环境中。核心目标是通过整合 Azure CNI 的控制平面和 Cilium 的 **eBPF** 数据平面，实现高吞吐、低延迟的网络架构，推动云计算领域的性能优化。

## 实施步骤  
1. 配置 Azure CNI 并启用 Cilium 数据平面：首先，在 Kubernetes 集群中部署 Azure CNI 插件，并整合 Cilium 作为数据平面组件，利用 **eBPF** 技术处理网络流量。这一步减少了传统网络处理的开销，提高了效率。理由在于 **eBPF** 允许在 Linux 内核中执行自定义代码，提供灵活性和性能提升。  
2. 启用 CiliumEndpointSlice 功能：在数据平面配置中激活 CiliumEndpointSlice，以批量处理 CiliumEndpoints 更新。这减少了对 Kubernetes 控制平面的负载，逻辑上衔接前一步的网络优化。通过这种批处理机制，避免了单个 pod 更新导致的性能瓶颈。  
3. 进行性能测试和优化：基于测试环境（如 1000 节点集群），监控指标如 API 服务器响应时间和 pod 启动延迟，并调整配置以适应实际 workload。技术原理涉及 **eBPF** 的高效包处理，确保在高并发场景下维持稳定性。

## 方案客户价值  
- **高性能网络提升**：通过 **eBPF** 实现高效包处理，显著降低网络延迟，例如平均 API 服务器响应时间从 1.5 秒减少 _50%_ 至 0.25 秒。这种收益源于减少控制平面负载，相比传统 CRD 管理方式，提供了更平滑的集群操作，但可能在极大规模场景下增加配置复杂性。  
- **增强安全和可观察性**：引入基于 DNS 的网络安全策略和高级可观察性工具，提升了流量管理和诊断能力，适用于高安全需求的环境。与传统 IaaS 方案相比，这降低了安全策略的部署门槛，但需注意 **eBPF** 依赖于特定内核版本，可能限制某些老旧系统的兼容性。  
- **量化业务收益**：例如，pod 启动延迟减少 _60%_，in-cluster 网络延迟降低 _80%_，这些通过减少资源浪费实现，帮助客户加速应用部署并提升用户体验。在突发流量场景中，该方案的优势显著，但若集群规模过大，可能面临管理开销增加的问题。

## 涉及的相关产品  
- **Azure CNI**：作为控制平面组件，负责 Kubernetes 网络策略的管理，在方案中确保与 Azure 生态的无缝集成，提供基础网络连接。  
- **Cilium**：基于 **eBPF** 的数据平面引擎，实现高性能网络和安全功能，是方案的核心，提升了 pod 通信效率。  
- **eBPF**：Linux 内核技术，支持自定义代码执行，在产品中用于优化包处理和监控，显著提高了整体网络性能。

## 技术评估  
该解决方案的技术先进性体现在 **eBPF** 的创新应用，实现高性能网络和安全策略自动化，适用于大规模分布式环境。可行性经性能测试验证，在 1000 节点集群中表现出色，优势包括显著降低延迟和控制平面负载，提升了 Kubernetes 的适用范围。然而，可能的局限性在于 **eBPF** 对内核版本的依赖，可能在非标准环境导致兼容问题；此外，在超大规模组网时，管理复杂度上升，可能需要额外工具辅助。同时，该方案顺应了云计算趋势向 Serverless 和微服务演进，但需关注 eBPF 实现的冷启动风险以优化实时性。

<!-- AI_TASK_END: AI竞争分析 -->


<!-- AI_TASK_START: AI全文翻译 -->
# 使用 Azure CNI 驱动的高扩展 Kubernetes 网络

**原始链接:** [https://techcommunity.microsoft.com/blog/azurenetworkingblog/high-scale-kubernetes-networking-with-azure-cni-powered-by-cilium/4407234](https://techcommunity.microsoft.com/blog/azurenetworkingblog/high-scale-kubernetes-networking-with-azure-cni-powered-by-cilium/4407234)  

**发布时间:** 2025-04-23  

**厂商:** AZURE  

**类型:** TECH-BLOG  

---  
Azure 网络博客  

# 使用 Azure CNI 驱动的高扩展 Kubernetes 网络  

2025 年 4 月 23 日  

Kubernetes (Kubernetes) 用户拥有多样化的集群网络需求，但其中最重要的是高效的 pod 网络和强大的安全功能。Azure CNI (Azure Container Networking Interface) 驱动由 Cilium 提供支持的解决方案，通过整合 Azure CNI 的控制平面和 Cilium 的 eBPF (extended Berkeley Packet Filter) 数据平面，满足这些需求。  

Cilium 通过利用 eBPF 的强大功能，实现高性能网络和安全。eBPF 是一种革命性的 Linux 内核技术，允许在 Linux 内核中执行自定义代码，从而提供灵活性和效率。这体现在：  

  * 高性能网络：eBPF 实现高效的数据包处理、降低延迟并提高吞吐量。  
  * 增强安全：Azure CNI (AzCNI) 驱动由 Cilium 提供支持，通过基于 DNS 的网络安全策略，轻松管理和保护网络流量（通过 [高级网络安全功能](<https://learn.microsoft.com/en-us/azure/aks/how-to-apply-fqdn-filtering-policies?tabs=cilium>)）。  
  * 更好的可观测性：我们的 eBPF 基础的 [高级网络可观测性套件](<https://learn.microsoft.com/en-us/azure/aks/container-network-observability-how-to?tabs=cilium>) 为集群用户提供详细的监控、追踪和诊断工具。  

## 介绍 CiliumEndpointSlice  

高效的 CNI 数据平面对于低延迟、高吞吐量的 pod 通信至关重要，从而提升分布式应用程序的效率和用户体验。虽然 Cilium 的 eBPF 驱动的数据平面目前已提供高性能网络，但我们寻求进一步提升其可扩展性和性能。为此，我们在数据平面的配置中启用了新功能 CiliumEndpointSlice (CiliumEndpointSlice)，从而实现：  

  * 降低 Kubernetes 控制平面的流量负载，导致控制平面内存消耗减少并提升性能。  
  * 更快地启动 pod 延迟。  
  * 更快地集群内网络延迟，从而改善应用程序性能。  

特别是，该功能改进了 Azure CNI 驱动由 Cilium 提供支持的管理 pod 方式。以前，Cilium 使用自定义资源定义 (CRDs) 称为 CiliumEndpoints 来管理 pod。每个 pod 都有一个关联的 CiliumEndpoint，其中包含 pod 的状态和属性信息。Cilium Agent（数据平面的核心组件）在每个节点上运行，并监视这些 CiliumEndpoints 以获取 pod 更新信息。我们观察到，这种行为会给控制平面带来重大压力和负载，尤其是在大型集群中，导致性能瓶颈。  

为了减轻控制平面的负载，我们引入了 CiliumEndpointSlice，该功能将 CiliumEndpoints 及其关联更新进行批量处理。这减少了传播到控制平面的更新数量。因此，我们大大降低了在规模化时超载控制平面的风险，确保集群更顺畅运行。  

### 性能测试  

我们对启用了和未启用 CiliumEndpointSlice 的 Azure CNI 驱动由 Cilium 提供支持进行了性能测试。测试在以下集群上进行：  
  * 1000 个节点 (Standard_D4_v3)。  
  * 20,000 个 pod（即每个节点 20 个 pod）。  
  * 1 个服务，带有 4000 个后端。  
  * 800 个服务，每个带有 20 个后端。  

测试涉及重复以下内容：  
#### 平均 API 服务器响应性  

此指标测量 kube-apiserver 对 LIST 请求（控制平面中最昂贵的请求类型之一）的平均延迟。启用 CiliumEndpointSlice 后，我们观察到延迟惊人地减少了 50%，从平均约 1.5 秒降至约 0.25 秒！对于集群用户，这意味着更快地处理发送到 kube-apiserver 的查询，从而提升性能。  

![](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/bS00NDA3MjM0LTJTWnZSRA?image-dimensions=798x322&revision=1)  

#### pod 启动延迟  

此指标测量 pod 被报告为运行所需的时间。启用 CiliumEndpointSlice 后，pod 启动延迟减少了超过 60%，允许更快地部署和扩展应用程序。  

#### 集群内网络延迟  

这是一个关键指标，测量从探测器 pod 到服务器的 ping 延迟。观察到延迟减少了超过 80%。这种延迟减少转化为更好的应用程序性能。  

![](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/bS00NDA3MjM0LUdOVGhvNg?image-dimensions=767x361&revision=1)  

Azure CNI 驱动由 Cilium 提供支持，为 Kubernetes 网络和安全提供强大的 eBPF 基础解决方案。从 Kubernetes 版本 1.32 开始启用 CiliumEndpointSlice。

<!-- AI_TASK_END: AI全文翻译 -->

