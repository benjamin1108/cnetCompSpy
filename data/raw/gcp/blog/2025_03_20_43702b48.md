# Using RDMA over Converged Ethernet networking for AI on Google Cloud

**原始链接:** [https://cloud.google.com/blog/products/networking/rdma-rocev2-for-ai-workloads-on-google-cloud](https://cloud.google.com/blog/products/networking/rdma-rocev2-for-ai-workloads-on-google-cloud)

**发布时间:** 2025-03-20

**厂商:** GCP

**类型:** BLOG

---
Networking

# 

Using RDMA over Converged Ethernet networking for AI on Google Cloud

March 20, 2025

  

![https://storage.googleapis.com/gweb-cloudblog-publish/images/0-hero-roce.max-2500x2500.png](https://storage.googleapis.com/gweb-cloudblog-publish/images/0-hero-roce.max-2500x2500.png)

##### Ammett Williams

Developer Relations Engineer

##### Google Cloud Next

On-demand access to Next’s top highlights.

[Watch now](https://cloud.withgoogle.com/next/25?utm_source=cgc-blog&utm_medium=blog&utm_campaign=FY25-Q2-global-EXP106-physicalevent-er-next25-mc&utm_content=cgc-blog-left-hand-rail-post-next&utm_term=-)

All workloads are not the same. This is especially the case for AI, ML, and scientific workloads. In this blog we show how Google Cloud makes the [RDMA over converged ethernet version 2 (RoCE v2)](https://en.wikipedia.org/wiki/RDMA_over_Converged_Ethernet) protocol available for high performance workloads. 

### **Traditional workloads**

Network communication in traditional workloads involves a well-known flow. This includes:

  * Movement of data between source and destination. The application initiates requests.

  * The OS processes the data, adds TCP headers and passes it to the network interface card (NIC).

  * The NIC sends data on the wire based on networking and routing information.

  * The Receiving NIC receives data. 

  * OS processing on the receiving end strips headers and delivers data based on information.

This process involves both CPU and [OS processing](https://en.wikipedia.org/wiki/Process_management_(computing)), and these networks can recover from latency and packet loss issues and handle data of varying sizes while functioning normally. 

### **AI workloads**

AI workloads are very sensitive, involve large datasets, may require high bandwidth, low latency and lossless communication for training and inference. Because there is a higher cost for running these types of jobs, it’s important that they are completed as quickly as possible and optimize processing. This can be achieved with accelerators — specialized hardware designed to significantly speed up the training and execution of AI applications. Examples of accelerators include specialized hardware chips like****[TPUs](https://cloud.google.com/tpu/docs/intro-to-tpu#how_a_tpu_works) and [GPUs](https://cloud.google.com/tpu/docs/intro-to-tpu#how_a_gpu_works).

### **RDMA**

[Remote Direct Memory Access](https://www.rfc-editor.org/rfc/rfc5040.html#section-1.1) (RDMA) technology allows systems to exchange data directly between one another without involving the OS, networking stack and CPU. This allows faster processing times since the CPU, which can become a bottleneck, is bypassed. 

Let's take a look at how this works with GPUs.

  * An RDMA-capable application initiates an RDMA operation.

  * Kernel bypass takes place, avoiding the OS and CPU.

  * RDMA-capable network hardware gets involved and accesses source GPU memory to transfer the data to the destination GPU memory. 

  * On the receiving end, the application can retrieve the information from the GPU memory, and a notification is sent to the sender as confirmation.

![https://storage.googleapis.com/gweb-cloudblog-publish/original_images/1-rdma-flow.gif](https://storage.googleapis.com/gweb-cloudblog-publish/original_images/1-rdma-flow.gif)

How RDMA with RoCE works

Previously, Google Cloud supported RDMA-like capabilities with its own native networking stack called [GPUDirect-TCPX](https://cloud.google.com/compute/docs/gpus/gpudirect) and [GPUDirect-TCPXO](https://cloud.google.com/cluster-toolkit/docs/machine-learning/a3-mega-enable-gpudirect-tcpxo). Currently the capability has been expanded with RoCEv2, which implements RDMA over ethernet.

### **RoCE-v2-capable compute**

Both the [A3 Ultra](https://cloud.google.com/ai-hypercomputer/docs/gpu#a3_ultra) and [A4](https://cloud.google.com/blog/products/compute/introducing-a4-vms-powered-by-nvidia-b200-gpu-aka-blackwell) Compute Engine machine types leverage RoCE v2 for high-performance networking. Each node supports eight RDMA-capable NICs connected to the isolated RDMA network. Direct GPU-to-GPU communication within a node occurs via NVLink and between nodes via RoCE.

Adopting RoCEv2 networking capabilities offers more benefits including:

  * Lower latency

  * Increased bandwidth — from 1.6 Tbps to 3.2 Tbps of inter-node GPU to GPU traffic

  * Lossless communication due to congestion management capabilities: [Priority-based Flow Control](https://1.ieee802.org/dcb/802-1qbb/) (PFC) and [Explicit Congestion Notification](https://datatracker.ietf.org/doc/html/rfc3168) (ECN)

  * Use of UDP port 4791

  * Support for new VM series like A3 Ultras, A4 and beyond

  * Scalability support for large cluster deployments

  * Optimized rail-designed network

![https://storage.googleapis.com/gweb-cloudblog-publish/original_images/2-rail-aligned.gif](https://storage.googleapis.com/gweb-cloudblog-publish/original_images/2-rail-aligned.gif)

rail design

Overall these features result in faster training and inference, directly improving application speed. It's achieved through a specialized VPC network, optimized for this purpose. This high-performance connectivity is a key differentiator for demanding applications.

### **Get started**

To enable these capabilities, follow these steps:

  1. [Create a reservation](https://cloud.google.com/ai-hypercomputer/docs/request-capacity): Obtain your reservation ID; you may have to work with your support team for capacity requests.

  2. [Choose a deployment strategy](https://cloud.google.com/ai-hypercomputer/docs/choose-strategy)**:** Specify the deployment region, zone, network profile, reservation ID and method.

  3. Create your deployment.

You can see the configuration steps and more in the following documentation:

  * Documentation: [Hypercompute Cluster](https://cloud.google.com/ai-hypercomputer/docs/create/create-overview)

  * Blog: [Cross-Cloud network support for AI workloads](https://cloud.google.com/blog/products/networking/cross-cloud-network-solutions-support-for-ai-workloads)

  * GCT YouTube Channel: [AI guide for Cloud Developers](https://www.youtube.com/playlist?list=PLIivdWyY5sqJio2yeg1dlfILOUO2FoFRx)

Want to ask a question, find out more or share a thought? Please connect with me on [Linkedin](https://www.linkedin.com/in/ammett/). 



Posted in

  * [Networking](https://cloud.google.com/blog/products/networking)
  * [Developers & Practitioners](https://cloud.google.com/blog/topics/developers-practitioners)